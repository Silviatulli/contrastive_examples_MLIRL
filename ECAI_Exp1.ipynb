{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6eXzKXL9J76b"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "ETA_Q = 5\n",
        "ETA_R = 5\n",
        "EPS = 1e-8\n",
        "STEP = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4GAQ9f1J76e"
      },
      "source": [
        "# The power of contrastive examples\n",
        "\n",
        "We compare RL, contrastive examples and demonstrations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgcJDZxsJ76g"
      },
      "source": [
        "## Environment 1 definition\n",
        "\n",
        "The environment looks like this:\n",
        "\n",
        "```\n",
        " ---- ---- ---- ---- ----\n",
        "|  1 |  2 |  3 |  4 |  5 |\n",
        " ---- ---- ---- ---- ----\n",
        "|  6 |    |  7 |    |  8 |\n",
        " ----      ----      ----\n",
        "|  9 |    | 10 |    | 11 |\n",
        " ----      ----      ----\n",
        "| 12 |    | 13 |    | 14 |\n",
        " ---- ---- ---- ---- ----\n",
        "| 15 | 16 | 17 | 18 | 19 |\n",
        " ---- ---- ---- ---- ----\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhM-GZkGJ76g",
        "outputId": "090e8fd6-783f-40ec-fbb0-beb18e71c83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "States: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n"
          ]
        }
      ],
      "source": [
        "S = list(map(lambda x : str(x), range(1, 20)))\n",
        "\n",
        "print('States:', S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlZgXAGoJ76h",
        "outputId": "18acf249-faab-4345-b726-7f3b9c0c0331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actions: ('Up', 'Down', 'Left', 'Right', 'No-Op')\n"
          ]
        }
      ],
      "source": [
        "A = ('Up', 'Down', 'Left', 'Right', 'No-Op')\n",
        "\n",
        "print('Actions:', A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zPS3eBhJ76i",
        "outputId": "2d70c7ce-a12b-4466-c780-f2761e18e21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition probabilities ok?\n",
            "\t- Action Up: True\n",
            "\t- Action Down: True\n",
            "\t- Action Left: True\n",
            "\t- Action Right: True\n",
            "\t- Action No-Op: True\n"
          ]
        }
      ],
      "source": [
        "#               1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19                \n",
        "Pu = np.array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 1\n",
        "               [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 2\n",
        "               [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 3\n",
        "               [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 4\n",
        "               [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 5\n",
        "               [.8, 0., 0., 0., 0., .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 6\n",
        "               [0., 0., .8, 0., 0., 0., .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 7\n",
        "               [0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 8\n",
        "               [0., 0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 9\n",
        "               [0., 0., 0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 10\n",
        "               [0., 0., 0., 0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0., 0., 0., 0., 0.],  # 11\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0., 0., 0., 0.],  # 12\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0., 0., 0.],  # 13\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0., 0.],  # 14\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, 0., 0., .2, 0., 0., 0., 0.],  # 15\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],  # 16\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, 0., 0., 0., .2, 0., 0.],  # 17\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],  # 18\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, 0., 0., 0., 0., .2]]) # 19\n",
        "\n",
        "#               1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19                \n",
        "Pd = np.array([[.2, 0., 0., 0., 0., .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 1\n",
        "               [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 2\n",
        "               [0., 0., .2, 0., 0., 0., .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 3\n",
        "               [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 4\n",
        "               [0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 5\n",
        "               [0., 0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 6\n",
        "               [0., 0., 0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 7\n",
        "               [0., 0., 0., 0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0., 0., 0., 0., 0.],  # 8\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0., 0., 0., 0.],  # 9\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0., 0., 0.],  # 10\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0., 0.],  # 11\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, 0., 0., .8, 0., 0., 0., 0.],  # 12\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, 0., 0., 0., .8, 0., 0.],  # 13\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, 0., 0., 0., 0., .8],  # 14\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],  # 15\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],  # 16\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],  # 17\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],  # 18\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]) # 19\n",
        "\n",
        "#               1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19                \n",
        "Pl = np.array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 1\n",
        "               [.8, .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 2\n",
        "               [0., .8, .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 3\n",
        "               [0., 0., .8, .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 4\n",
        "               [0., 0., 0., .8, .2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 5\n",
        "               [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 6\n",
        "               [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 7\n",
        "               [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 8\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 9\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 10\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],  # 11\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],  # 12\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],  # 13\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],  # 14\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],  # 15\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, .2, 0., 0., 0.],  # 16\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, .2, 0., 0.],  # 17\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, .2, 0.],  # 18\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .8, .2]]) # 19\n",
        "\n",
        "#               1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19                \n",
        "Pr = np.array([[.2, .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 1\n",
        "               [0., .2, .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 2\n",
        "               [0., 0., .2, .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 3\n",
        "               [0., 0., 0., .2, .8, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 4\n",
        "               [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 5\n",
        "               [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 6\n",
        "               [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 7\n",
        "               [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 8\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 9\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 10\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],  # 11\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],  # 12\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],  # 13\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],  # 14\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, .8, 0., 0., 0.],  # 15\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, .8, 0., 0.],  # 16\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, .8, 0.],  # 17\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., .2, .8],  # 18\n",
        "               [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]) # 19\n",
        "\n",
        "Pn = np.eye(len(S))\n",
        "\n",
        "P = [Pu, Pd, Pl, Pr, Pn]\n",
        "\n",
        "print('Transition probabilities ok?')\n",
        "\n",
        "for aidx in range(len(A)):\n",
        "    print('\\t- Action %s:' % A[aidx], np.all(np.isclose(P[aidx].sum(axis=1), 1.)))\n",
        "    \n",
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "utyA_tD3J76k"
      },
      "outputs": [],
      "source": [
        "# Rewards are a convex combination of 7 features/rewards\n",
        "CELL_GOAL = np.zeros((len(S), len(A)))\n",
        "CELL_GOAL[2, :] = 1\n",
        "\n",
        "CELL_9  = np.zeros((len(S), len(A)))\n",
        "CELL_9[8, :] = 1\n",
        "\n",
        "CELL_10 = np.zeros((len(S), len(A)))\n",
        "CELL_10[9, :] = 1\n",
        "\n",
        "CELL_11 = np.zeros((len(S), len(A)))\n",
        "CELL_11[10, :] = 1\n",
        "\n",
        "RFEAT = (CELL_GOAL, CELL_9, CELL_10, CELL_11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcthHE-sJ76k"
      },
      "source": [
        "## MDP solver\n",
        "\n",
        "The next function is used to compute the optimal $Q$-function for MDPs. For computational efficiency, we pre-compute the optimal $Q$-function for all MDPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KOVzIT71J76l"
      },
      "outputs": [],
      "source": [
        "def vi(M):\n",
        "    ''' \n",
        "    Compute optimal Q-function for MDP M.\n",
        "\n",
        "    :param M: 5-tuple corresponding to MDP description: (S, A, P, R, gamma)\n",
        "    :return: np.array with shape (|S|, |A|), corresponding to Q* for M.\n",
        "    '''\n",
        "\n",
        "    S = M[0]\n",
        "    A = M[1]\n",
        "    P = M[2]\n",
        "    R = M[3]\n",
        "    gamma = M[4]\n",
        "    \n",
        "    Q = np.zeros((len(S), len(A)))\n",
        "    quit = False\n",
        "    niter = 0\n",
        "    \n",
        "    while not quit:\n",
        "        Qnew = np.zeros((len(S), len(A)))\n",
        "        V = np.max(Q, axis=1, keepdims=True)\n",
        "        for a in range(len(A)):\n",
        "            Qnew[:, a, None] = R[:, a, None] + gamma * P[a].dot(V)\n",
        "            \n",
        "        if np.linalg.norm(Q - Qnew) < EPS:\n",
        "            quit = True\n",
        "            \n",
        "        Q = Qnew\n",
        "        niter += 1\n",
        "        \n",
        "    # print('Finished after %i iterations.' % niter)\n",
        "    return Q\n",
        "\n",
        "# --\n",
        "\n",
        "def greedy(Q):\n",
        "    ''' \n",
        "    Compute greedy policy with respect to Q.\n",
        "    \n",
        "    :param Q: np.array with shape (|S|, |A|).\n",
        "    :return: np.array with shape (|S|, |A|), corresponding to pi_g(Q).\n",
        "    '''\n",
        "    \n",
        "    pol = (Q == np.max(Q, axis=1, keepdims=True)).astype(int)\n",
        "    pol = pol / np.sum(pol, axis=1, keepdims=True)\n",
        "\n",
        "    return pol\n",
        "\n",
        "# --\n",
        "\n",
        "def boltzmann(Q):\n",
        "    ''' \n",
        "    Computes the Boltzmann policy with respect to Q.\n",
        "\n",
        "    :param Q: np.array with shape (|S|, |A|).\n",
        "    :return: np.array with shape (|S|, |A|) corresponding to pi_B(Q).\n",
        "    '''\n",
        "    \n",
        "    Q = ETA_Q * (Q - np.max(Q, axis=1, keepdims=True))\n",
        "\n",
        "    pol = np.round(np.exp(Q), 3)\n",
        "    pol = pol / np.sum(pol, axis=1, keepdims=True)\n",
        "    \n",
        "    return pol\n",
        "\n",
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M462WAssJ76m"
      },
      "outputs": [],
      "source": [
        "w = np.random.randn(4)\n",
        "r = np.sum([RFEAT[m] * w[m] for m in range(len(RFEAT))], axis=0)\n",
        "M = (S, A, P, r, gamma)\n",
        "Q = vi(M)\n",
        "pol = boltzmann(Q)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5N1L7v66D7h",
        "outputId": "de33e57c-b908-43b0-8a0f-fc6aff1a6471"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.7744733 , 0.7744733 , 0.7744733 , 0.7744733 , 0.7744733 ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.45640399, 0.45640399, 0.45640399, 0.45640399, 0.45640399],\n",
              "       [0.85525387, 0.85525387, 0.85525387, 0.85525387, 0.85525387],\n",
              "       [0.37049406, 0.37049406, 0.37049406, 0.37049406, 0.37049406],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHt6k7pbJ76m"
      },
      "source": [
        "## RL updater\n",
        "\n",
        "We perform maximum likelihood estimation using SGD, where \n",
        "\n",
        "$$r_t\\sim\\mathrm{Normal}(w^\\top\\phi(s_t,a_t);\\eta)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gGr9dYM7J76m"
      },
      "outputs": [],
      "source": [
        "# Weight update\n",
        "def rl_update(wold, s, a, r):\n",
        "    ''' \n",
        "    Updates the weights wold given the triplet (s, a, r) and the weight w. \n",
        "    The update is a Stochastic Gradient Descent (SGD) update on the negative log likelihood, assuming the\n",
        "    reward is disturbed by 0-mean Gaussian noise.\n",
        "    \n",
        "    :param wold: np.array with shape (nfeat,)\n",
        "    :param s: int (state index)\n",
        "    :param a: int (action index)\n",
        "    :param r: float (reward)\n",
        "    :return: np.array with shape (nfeat,)\n",
        "    '''\n",
        "    \n",
        "    feat = np.array([RFEAT[m][s, a] for m in range(len(RFEAT))])\n",
        "    \n",
        "    grad = feat * (feat.dot(wold) - r)    \n",
        "    wnew = wold - STEP * grad\n",
        "\n",
        "    return wnew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bLtdUSwJ76n"
      },
      "source": [
        "## Demonstration updater\n",
        "\n",
        "We perform maximum likelihood estimation using SGD, where \n",
        "\n",
        "$$\\mathbb{P}[a_t\\mid s_t]=\\frac{\\exp\\{\\eta Q^*_w(s,a)\\}}{\\sum_{a'}\\exp\\{\\eta_Q Q^*_w(s,a')\\}},$$\n",
        "\n",
        "where $Q^*_w$ is the optimal $Q$-function for the MDP with reward $w^\\top\\phi$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O-ltVWL7J76n"
      },
      "outputs": [],
      "source": [
        "def policy_feat(pol, feat):\n",
        "    '''\n",
        "    Compute the policy-weighted feature matrix. \n",
        "    \"pol\" can be a single action index or a full \n",
        "    |S| x |A| policy matrix.\n",
        "    \n",
        "    :param pol: int (action index) or np.array with shape (|S|, |A|) (policy)\n",
        "    :param feat: tuple (each element is a (|S|, |A|)-shaped np.array)\n",
        "    :return: np.array with shape (|S|, |nfeat|)\n",
        "    '''\n",
        "    \n",
        "    if isinstance(pol, int):\n",
        "        return np.hstack([feat[m][:, pol, None] for m in range(len(feat))])\n",
        "    \n",
        "    else:\n",
        "        return np.hstack([np.sum(feat[m] * pol, axis=1, keepdims=True) for m in range(len(feat))])\n",
        "    \n",
        "# --\n",
        "\n",
        "def policy_trans(pol, P):\n",
        "    '''\n",
        "    Compute the policy-weighted transition probabilities.\n",
        "    \n",
        "    :param pol: np.array with shape (|S|, |A|) (policy)\n",
        "    :param P: tuple (each element is a (|S|, |S|)-shaped np.array)\n",
        "    :return: np.array with shape (|S|, |S|)\n",
        "    '''\n",
        "    \n",
        "    return np.sum([np.diag(pol[:, a]).dot(P[a]) for a in range(len(A))], axis=0)\n",
        "\n",
        "# --\n",
        "\n",
        "def policy_rew(pol, r):\n",
        "    '''\n",
        "    Compute the policy-weighted reward.\n",
        "    \n",
        "    :param pol: np.array with shape (|S|, |A|) (policy)\n",
        "    :param r: np.array with shape (|S|, |A|) (reward)\n",
        "    :return: np.array with shape (|S|,)\n",
        "    '''\n",
        "    \n",
        "    return np.sum(pol * r, axis=1)\n",
        "\n",
        "# --\n",
        "\n",
        "def discounted_freq(pol, P):\n",
        "    '''\n",
        "    Compute the discounted visitation frequencies given a policy pol.\n",
        "    \n",
        "    :param pol: np.array with shape (|S|, |A|) (policy)\n",
        "    :param P: tuple (each element is a (|S|, |S|)-shaped np.array)\n",
        "    :return: np.array with shape (|S|, |S|)\n",
        "    '''\n",
        "    \n",
        "    Ppol = policy_trans(pol, P)\n",
        "    return (1 - gamma) * np.linalg.inv(np.eye(len(S)) - gamma * Ppol)\n",
        "\n",
        "# --\n",
        "    \n",
        "# Weight update\n",
        "def demo_update(wold, s, a, pol=None):\n",
        "    ''' \n",
        "    Updates the weights wold given the pair (s, a) and the weight wold. \n",
        "    The update is a SGD update on the negative log likelihood, assuming the\n",
        "    actions are selected from a Boltzmann distribution (pol, which can be\n",
        "    precomputed).\n",
        "    \n",
        "    :param wold: np.array with shape (|nfeat|,)\n",
        "    :param s: int (state index)\n",
        "    :param a: int (action index)\n",
        "    :param pol: np.array with shape (|S|, |A|) (policy)\n",
        "    :return: np.array with shape (nfeat,)\n",
        "    '''\n",
        "\n",
        "    if pol is None:\n",
        "        rhat = np.sum([RFEAT[m] * w[m] for m in range(len(RFEAT))], axis=0)\n",
        "        M = (S, A, P, rhat, gamma)\n",
        "        Q = vi(M)\n",
        "        pol = boltzmann(Q)\n",
        "\n",
        "    feat = policy_feat(greedy(Q), RFEAT) # (nS, nF)\n",
        "    disc = discounted_freq(greedy(Q), P) # (nS, nS)\n",
        "    \n",
        "    grad = np.zeros(wold.shape)\n",
        "\n",
        "    for a_idx in range(len(A)):\n",
        "              #              (nS, nF)                     (nS, nS) (nS, nS) (nS, nF)\n",
        "        grad += ETA_Q * (policy_feat(a_idx, RFEAT) + gamma * P[a_idx].dot(disc).dot(feat))[s, :] * (pol[s, a_idx] - int(a == a_idx))\n",
        "        \n",
        "    # Gradients are too small, so we push the step-size up\n",
        "    \n",
        "    wnew = wold - 2 * STEP * grad\n",
        "        \n",
        "    return wnew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQFCuICrJ76n",
        "outputId": "2c6981b9-d695-4e96-918b-60ecd0a16e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_feat(.): (19, 4)\n",
            "policy_feat(.): (19, 4)\n",
            "policy_trans(.): (19, 19)\n",
            "policy_rew(.): (19,)\n",
            "discounted_freq(.): (19, 19)\n"
          ]
        }
      ],
      "source": [
        "print('policy_feat(.):', policy_feat(pol, RFEAT).shape)\n",
        "print('policy_feat(.):', policy_feat(0, RFEAT).shape)\n",
        "print('policy_trans(.):', policy_trans(pol, P).shape)\n",
        "print('policy_rew(.):', policy_rew(pol, r).shape)\n",
        "print('discounted_freq(.):', discounted_freq(pol, P).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7k45CqBJ76o"
      },
      "source": [
        "## Contrastive examples updater\n",
        "\n",
        "We consider the following contrastive examples:\n",
        "\n",
        "- In state $s$ action $a$ is better than action $a'$ because it will eventually lead you through state $s'$ and that is good.\n",
        "- In state $s$ action $a$ is worse than action $a'$ because it will eventually lead you through state $s'$ and that is bad.\n",
        "\n",
        "We consider that, given the target reward $r$,\n",
        "\n",
        "- A state $s$ is good if $r^*(s)>0$ for the optimal policy $\\pi^*_r$ given that reward.\n",
        "- A state $s$ is bad if $r^*(s)<0$ for the optimal policy $\\pi^*_r$ given that reward.\n",
        "- An action $a$ is _better_ than an action $a'$ in state $s$ and leading to state $s'$ if the following two conditions are cumulatively met:\n",
        "\n",
        " - $Q^*(s,a)>Q^*(s,a')$ ($a$ is better than $a'$)\n",
        "    * The transition probabilities by first taking action $a$ and then following $\\pi^*$ lead to larger transition probability from $s$ to $s'$ than those same probabilities taking action $a'$.\n",
        "    \n",
        "- An action $a$ is _worse_ than an action $a'$ in state $s$ and leading to state $s'$ if the following two conditions are cumulatively met:\n",
        "\n",
        " - $Q^*(s,a)<Q^*(s,a')$ ($a$ is worse than $a'$)\n",
        "    * The transition probabilities by taking action $a$ in $s$ and following $\\pi^*$ elsewhere lead to larger transition probability from $s$ to $s'$ than those same probabilities taking action $a'$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MTFJKY7ZJ76o"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z, eta):\n",
        "    ''' \n",
        "    Compute sigmoid function.\n",
        "    \n",
        "    :param z: float\n",
        "    :param eta: float\n",
        "    :return: float\n",
        "    '''\n",
        "    \n",
        "    return 1 / (1 + np.exp(-eta * z))\n",
        "\n",
        "# --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ4Px7V8J76o"
      },
      "source": [
        "We perform maximum likelihood estimation using SGD, where \n",
        "\n",
        "$$\\mathbb{P}[(s,a,a',s',k)\\mid w]=\\sigma(kr_w(s'))\\sigma(k(Q(s,a)-Q(s,a')))\\mathbf{P}_{a}^\\infty(s'\\mid s)$$\n",
        "\n",
        "(see precise definition in the paper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Uq4h5tnFJ76p"
      },
      "outputs": [],
      "source": [
        "# Weight update\n",
        "def explain_update(wold, s0, s1, a0, a1, k, Q=None):\n",
        "    '''\n",
        "    Updates the weights wold given the tuple (s0, s1, a0, a1, k) \n",
        "    and the weight wold. The update is a SGD update on the negative \n",
        "    log likelihood, assuming the distribution above.\n",
        "    '''\n",
        "    \n",
        "    # Compute policy\n",
        "    r = np.sum([RFEAT[m] * wold[m] for m in range(len(RFEAT))], axis=0)\n",
        "\n",
        "    if Q is None:\n",
        "        M = (S, A, P, r, gamma)\n",
        "        Q = vi(M)\n",
        "        \n",
        "    pol = boltzmann(Q)\n",
        "\n",
        "    paux = pol.copy()\n",
        "    paux[s0] = np.zeros(len(A))\n",
        "    paux[s0, a0] = 1.0\n",
        "        \n",
        "    # Component 1 - reward\n",
        "    fpol = policy_feat(greedy(Q), RFEAT)      # (nS, nF)\n",
        "    rpol = policy_rew(greedy(Q), r) # (nS,)\n",
        "\n",
        "    grad_R = ETA_R * k * fpol[s1, :] * (sigmoid(k * rpol[s1], ETA_R) - 1) # (nF,)\n",
        "    #print('grad_R:')\n",
        "    #print(grad_R)\n",
        "    \n",
        "    # Component 2 - Q-values\n",
        "    fa0 = policy_feat(a0, RFEAT)   # (nS, nF)\n",
        "    fa1 = policy_feat(a1, RFEAT)   # (nS, nF)\n",
        "    freq = discounted_freq(greedy(Q), P) # (nS, nS)\n",
        "\n",
        "    diff_Q = Q[s0, a0] - Q[s0, a1]       # scalar\n",
        "    diff_P = P[a0][s0, :] - P[a1][s0, :] # (nS,)\n",
        "    diff_F = fa0[s0] - fa1[s0]           # (nF,)\n",
        "    \n",
        "    grad_Q = ETA_Q * k * (diff_F + gamma * diff_P.dot(freq).dot(fpol)) * (sigmoid(k * diff_Q, ETA_Q) - 1) # (nF,)\n",
        "    #print('grad_Q:')\n",
        "    #print(grad_Q)\n",
        "\n",
        "    # Component 3 - Pam\n",
        "    Pam = discounted_freq(paux, P) # (nS, nS)\n",
        "    pf = Pam.dot(fpol)             # (nS, nF)\n",
        "    \n",
        "    grad_P = np.zeros(len(RFEAT))\n",
        "    \n",
        "    for k in range(len(RFEAT)):\n",
        "        # for a single k\n",
        "        grad_aux = np.zeros((len(S), len(S)))\n",
        "\n",
        "        for a in range(len(A)):\n",
        "            for ap in range(len(A)):\n",
        "                fa  = policy_feat(a, RFEAT)  # (nS, nF)\n",
        "                fap = policy_feat(ap, RFEAT) # (nS, nF)\n",
        "                    #  (nS,)            (nS, nS) (nS, nS) (nS,)\n",
        "                dT  = fap[:, k] + gamma * P[ap].dot(freq).dot(fpol[:, k]) # (nS,)\n",
        "                tmp = ETA_Q * np.diag(paux[:, a] * dT * (pol[:, ap] - int(a == ap))).dot(P[a]) # (nS, nS)\n",
        "\n",
        "                grad_aux += tmp\n",
        "\n",
        "        dPk = Pam.dot(grad_aux).dot(Pam)      # (nS, nS)\n",
        "        if np.isclose(Pam[s0, s1], 0) and not np.isclose(dPk[s0, s1], 0):\n",
        "            print('Whoa! Zero!')\n",
        "            print('dPk:', dPk[s0, s1])\n",
        "\n",
        "        elif np.isclose(Pam[s0, s1], 0):\n",
        "            grad_P[k] = 0.                        # scalar\n",
        "\n",
        "        else:\n",
        "            grad_P[k] = dPk[s0, s1] / Pam[s0, s1] # scalar\n",
        "        \n",
        "    #print('grad_P:')\n",
        "    #print(grad_P)\n",
        "    \n",
        "    grad = grad_R + grad_Q + grad_P\n",
        "    \n",
        "    wnew = wold - STEP * grad\n",
        "    \n",
        "    return wnew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHi5ZyhkJ76p",
        "outputId": "d8dc972b-b5c1-4adc-b327-d66c29f99e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P[a][s, :]: (19,)\n",
            "freq: (19, 19)\n",
            "fpol: (19, 4)\n",
            "P[a][s, :].dot(freq): (19,)\n",
            "P[a][s, :].dot(freq).dot(fpol): (4,)\n"
          ]
        }
      ],
      "source": [
        "freq = discounted_freq(pol, P)\n",
        "fpol = policy_feat(pol, RFEAT)\n",
        "print('P[a][s, :]:', P[0][0, :].shape)\n",
        "print('freq:', freq.shape)\n",
        "print('fpol:', fpol.shape)\n",
        "print('P[a][s, :].dot(freq):', P[0][0, :].dot(freq).shape)\n",
        "print('P[a][s, :].dot(freq).dot(fpol):', P[0][0, :].dot(freq).dot(fpol).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b_HimgRJ76p"
      },
      "source": [
        "## Experiments\n",
        "\n",
        "We now generate a random reward, and compare the performance of an RL updater, a demo updater and an contrastive examples updater in randomly selected sample rewards, demos and contrastive examples.\n",
        "\n",
        "We start by defining the samplers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EaY9kXR1J76p"
      },
      "outputs": [],
      "source": [
        "def sample_reward(M):\n",
        "    ''' Samples a reward for MDP M. '''\n",
        "\n",
        "    S = M[0]\n",
        "    A = M[1]\n",
        "    R = M[3]\n",
        "    \n",
        "    # Sample a random state\n",
        "    sample_s = np.random.choice(len(S))\n",
        "    print(sample_s)\n",
        "    \n",
        "    # Sample a random action\n",
        "    sample_a = np.random.choice(len(A))\n",
        "    print(sample_a)\n",
        "    \n",
        "    # Sample a noisy reward\n",
        "    sample_r = R[sample_s, sample_a] + np.random.randn() / ETA_R\n",
        "    print(sample_r)\n",
        "    \n",
        "    return (sample_s, sample_a, sample_r)\n",
        "\n",
        "# -- \n",
        "\n",
        "def voice_reward(M, s, a, r):\n",
        "    ''' Converts a reward sample into a natural language string. '''\n",
        "    \n",
        "    return 'The reward in state %s when performing action %s is %.2f.' % (M[0][s], M[1][a], r)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = sample_reward(M)"
      ],
      "metadata": {
        "id": "SjABUVGDED3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453b11b6-1db2-4b25-93ac-37eb00aa945c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "0\n",
            "0.14084202040055693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NlSSm3f0J76q",
        "outputId": "6cb7c26d-e564-4823-a780-4cd3fa1a0051"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The reward in state 11 when performing action Up is 0.14.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "voice_reward(M, *example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YJNptXPOJ76q"
      },
      "outputs": [],
      "source": [
        "def sample_demo(M):\n",
        "    ''' Samples a demonstration for MDP M. '''\n",
        "\n",
        "    S = M[0]\n",
        "    A = M[1]\n",
        "    \n",
        "    # Sample a random state\n",
        "    sample_s = np.random.choice(len(S))\n",
        "    print(sample_s)\n",
        "\n",
        "    # Sample a noisy action\n",
        "    Q = vi(M)\n",
        "    pol = boltzmann(Q)[sample_s, :]\n",
        "    sample_a = np.random.choice(len(A), p=pol)\n",
        "    print(sample_a)\n",
        "    \n",
        "    return (sample_s, sample_a)\n",
        "\n",
        "# --\n",
        "\n",
        "def voice_demo(M, s, a):\n",
        "    ''' Converts a demonstration sample into a natural language string. '''\n",
        "\n",
        "    return 'In state %s you should perform action %s.' % (M[0][s], M[1][a])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = sample_demo(M)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpiBjKR1LU5l",
        "outputId": "80362e1b-18cc-42b3-957e-c52bee78b53c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voice_demo(M, *example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OWhuHNPMLlNm",
        "outputId": "9e5a8962-ba59-4f34-da91-7b8f147802f6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In state 18 you should perform action Left.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "6G6YXh88J76q",
        "outputId": "b7c07e9f-35fe-4f92-9e1a-72b965858f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "9\n",
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In state 10 you should perform action Left.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "Q = vi(M)\n",
        "pol = boltzmann(Q)\n",
        "print(pol.sum(axis=1))\n",
        "voice_demo(M, *sample_demo(M))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "s_RXa80KJ76q"
      },
      "outputs": [],
      "source": [
        "def sample_explain(M, echo=False):\n",
        "    ''' Samples an explanation for MDP M. '''\n",
        "\n",
        "    S = M[0]\n",
        "    A = M[1]\n",
        "    P = M[2]\n",
        "    R = M[3]\n",
        "    gamma = M[4]\n",
        "    \n",
        "    Q = vi(M)\n",
        "    pol = boltzmann(Q)\n",
        "    rpol = policy_rew(pol, R)\n",
        "\n",
        "    sample_complete = False\n",
        "    \n",
        "    while not sample_complete:\n",
        "        \n",
        "        # This ensures that the search for next state doesn't go on forever\n",
        "        stop_sample = False\n",
        "        \n",
        "        # Sample a random initial state\n",
        "        sample_s0 = np.random.choice(len(S))\n",
        "        \n",
        "        if echo:\n",
        "            print('Sampled state %s' % S[sample_s0])\n",
        "\n",
        "        # Sample a random initial action\n",
        "        sample_a0 = np.random.choice(len(A))\n",
        "        \n",
        "        if echo:\n",
        "            print('Sampled action %s' % A[sample_a0])\n",
        "\n",
        "        paux = pol.copy()\n",
        "        paux[sample_s0] = np.zeros(len(A))\n",
        "        paux[sample_s0, sample_a0] = 1.0\n",
        "        \n",
        "        Pam = discounted_freq(paux, P)\n",
        "\n",
        "        # Sample a random second action (different from the first)\n",
        "        quit = False\n",
        "        while not quit:\n",
        "            sample_a1 = np.random.choice(len(A))\n",
        "            quit = sample_a1 != sample_a0\n",
        "\n",
        "        if echo:\n",
        "            print('Sampled another action: %s' % A[sample_a1])\n",
        "\n",
        "        # Sample a random via state with non-zero reward (to fit the \"good\" or \"bad\" description)\n",
        "\n",
        "        quit = False\n",
        "        while not quit:\n",
        "            paux = np.maximum(rpol * (Q[sample_s0, sample_a0] - Q[sample_s0, sample_a1]) * Pam[sample_s0, :], 0)\n",
        "\n",
        "            # Ensure that there is a next state to sample\n",
        "            if np.isclose(np.sum(paux), 0):\n",
        "                if echo:\n",
        "                    print('Zero support distribution. Restarting sampling.')\n",
        "                stop_sample = True\n",
        "                quit = True\n",
        "            else:\n",
        "                paux = paux / paux.sum()\n",
        "                if np.isclose(paux[sample_s0], 1.):\n",
        "                    if echo:\n",
        "                        print('Departing from sink. Restarting sampling.')\n",
        "                    stop_sample = True\n",
        "                    quit = True\n",
        "                else:\n",
        "                    if echo:\n",
        "                        print(np.round(paux, 3))\n",
        "                    sample_s1 = np.random.choice(len(S), p=paux)\n",
        "                    quit = (sample_s1 != sample_s0)\n",
        "\n",
        "        if not stop_sample:\n",
        "            if echo:\n",
        "                print('Sampled another state: %s' % S[sample_s1])\n",
        "\n",
        "            # Check which one is better\n",
        "            if Q[sample_s0, sample_a0] > Q[sample_s0, sample_a1]:\n",
        "                k = 1\n",
        "            else:\n",
        "                k = -1\n",
        "                \n",
        "            sample_complete = True\n",
        "            \n",
        "    if echo:\n",
        "        print('Q(%s, %s) =' % (S[sample_s0], A[sample_a0]), Q[sample_s0, sample_a0])\n",
        "        print('Q(%s, %s) =' % (S[sample_s0], A[sample_a1]), Q[sample_s0, sample_a1])\n",
        "        print('reward(%s) =' % S[sample_s1], np.sum(R * pol, axis=1)[sample_s1])\n",
        "        print('P_m[%s](%s, %s) =' % (A[sample_a0], S[sample_s0], S[sample_s1]), Pam[sample_s0, sample_s1])\n",
        "        \n",
        "    return (sample_s0, sample_s1, sample_a0, sample_a1, k)\n",
        "\n",
        "# --\n",
        "        \n",
        "def voice_explain(M, s0, s1, a0, a1, k):\n",
        "    ''' Converts an exmplanation sample into a natural language string. '''\n",
        "\n",
        "    REL1 = ('better', 'worse')\n",
        "    REL2 = ('good', 'bad')\n",
        "    \n",
        "    k = 1 - (k + 1) // 2\n",
        "    \n",
        "    return 'In state %s, action %s is %s than %s because it may eventually lead you through state %s and that is %s.' % (M[0][s0], M[1][a0], REL1[k], M[1][a1], M[0][s1], REL2[k])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = sample_explain(M, echo=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmC88r5fNrTr",
        "outputId": "7b2acce2-9f40-462f-b2dc-e51d55411efb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled state 19\n",
            "Sampled action Left\n",
            "Sampled another action: Down\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sampled another state: 10\n",
            "Q(19, Left) = 81.33890138453003\n",
            "Q(19, Down) = 80.52551236967065\n",
            "reward(10) = 0.8552538730247325\n",
            "P_m[Left](19, 10) = 0.939841885409379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voice_explain(M, *example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E1FTTpiTNvCi",
        "outputId": "ca5b5be4-14de-40c4-e8c0-e633ba952df4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In state 19, action Left is better than Down because it may eventually lead you through state 10 and that is good.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "yQLwYih7J76r",
        "scrolled": false,
        "outputId": "d0cfc467-5009-4635-dddb-bf5813f859c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled state 11\n",
            "Sampled action Right\n",
            "Sampled another action: Left\n",
            "Zero support distribution. Restarting sampling.\n",
            "Sampled state 3\n",
            "Sampled action Up\n",
            "Sampled another action: Down\n",
            "Zero support distribution. Restarting sampling.\n",
            "Sampled state 13\n",
            "Sampled action No-Op\n",
            "Sampled another action: Down\n",
            "Zero support distribution. Restarting sampling.\n",
            "Sampled state 17\n",
            "Sampled action Down\n",
            "Sampled another action: No-Op\n",
            "Zero support distribution. Restarting sampling.\n",
            "Sampled state 11\n",
            "Sampled action Down\n",
            "Sampled another action: Up\n",
            "Zero support distribution. Restarting sampling.\n",
            "Sampled state 17\n",
            "Sampled action Up\n",
            "Sampled another action: Left\n",
            "[-0.  0.  0.  0.  0. -0.  0.  0. -0.  1.  0. -0.  0. -0. -0. -0.  0.  0.\n",
            "  0.]\n",
            "Sampled another state: 10\n",
            "Q(17, Up) = 83.40588129998162\n",
            "Q(17, Left) = 81.74816340778943\n",
            "reward(10) = 0.8552538730247325\n",
            "P_m[Up](17, 10) = 0.9646626028114518\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In state 17, action Up is better than Left because it may eventually lead you through state 10 and that is good.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "voice_explain(M, *sample_explain(M, echo=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5_t25yaJ76r"
      },
      "source": [
        "We now run a comparative study. Every 10 steps, for each of the three approaches, we:\n",
        "\n",
        "* Estimate the reward;\n",
        "* Compute the associated \"optimal\" policy;\n",
        "* Evaluate that policy in the correct MDP.\n",
        "\n",
        "The next function is used in the last step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1K4B1KnTJ76r"
      },
      "outputs": [],
      "source": [
        "def evaluate_reward(M, w):\n",
        "    ''' Computes the policy from weights w in the MDP M.'''\n",
        "\n",
        "    S = M[0]\n",
        "    A = M[1]\n",
        "    P = M[2]\n",
        "    R = M[3]\n",
        "    gamma = M[4]\n",
        "    \n",
        "    Raux = np.sum([w[i] * RFEAT[i] for i in range(len(RFEAT))], axis=0)\n",
        "    \n",
        "    Maux = (S, A, P, Raux, gamma)\n",
        "    pol  = greedy(vi(Maux))\n",
        "    \n",
        "    Ppol = policy_trans(pol, P)\n",
        "    Rpol = policy_rew(pol, R)\n",
        "    \n",
        "    Vaux = np.linalg.inv(np.eye(len(S)) - gamma * Ppol).dot(Rpol)\n",
        "\n",
        "    # Compute average value\n",
        "    return Vaux.sum() / len(S)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUCphrdDwT-",
        "outputId": "d6c3b9b9-85ed-49df-f4f2-2a02cd1dc4a2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzbntx4sJ76r"
      },
      "source": [
        "We can now run our comparative study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9b9c028b12b74788b8502fd92ecfa124",
            "3e53844bfc6749d5a922993b572b252f",
            "21ae086e0d964e07ab01977a3a0815c7",
            "3c1fd03ed1014f44bb97971e49053611",
            "93c59174246b4ac9ab8a6de1b53eb591",
            "ac2cbfefa53c4b3f93d79f97cb69c5ff",
            "f1e050c638dc4a5f8f2abd6786798fce",
            "730fd69f7f644a9fbf90f8f30a1fface",
            "dcfc92b369234d22bf5db17165ff6774",
            "572d23dc8b98451098ddfa7dd13454e5",
            "990567b6d90d401c896ef2af9cc26e8d"
          ]
        },
        "id": "KcBq_eJKJ76s",
        "scrolled": false,
        "outputId": "e2afc00e-7e25-4f93-8980-235a46f87117"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b9c028b12b74788b8502fd92ecfa124"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.4939049   0.57084784 -0.02108904  0.00171039]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is worse than Down because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "2\n",
            "0.07552954128158537\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Left is 0.08.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00044194  0.01969441 -0.03368271  0.        ]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.4939049   0.62591545 -0.02108904  0.00171039]\n",
            "Evaluating reward...\n",
            "In state 16, action Left is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "17\n",
            "3\n",
            "-0.05550030464670983\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 18 when performing action Right is -0.06.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00091416  0.01969441 -0.07061634  0.        ]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.4939049   0.66788875 -0.02111513  0.00171039]\n",
            "Evaluating reward...\n",
            "In state 13, action Down is better than Up because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "4\n",
            "-0.244927613162983\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action No-Op is -0.24.\n",
            "\n",
            "Sampling demo... \n",
            "12\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00091416  0.01969441 -0.10902082  0.        ]\n",
            "Evaluating reward...\n",
            "In state 13 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49391403  0.70250939 -0.02151398  0.00171039]\n",
            "Evaluating reward...\n",
            "In state 10, action Down is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "13\n",
            "4\n",
            "0.0026867383959279256\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 14 when performing action No-Op is 0.00.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.01952758  0.01969441 -0.10441061 -0.0049087 ]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49390923  0.73183614 -0.02151398  0.00171039]\n",
            "Evaluating reward...\n",
            "In state 5, action Left is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "0\n",
            "0.03335927134880677\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Up is 0.03.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.01951644  0.01969441 -0.10353942 -0.0049087 ]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49134383  0.75928534 -0.02400524  0.00171039]\n",
            "Evaluating reward...\n",
            "In state 10, action Up is better than Right because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "8\n",
            "3\n",
            "2.4926024176539108\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.49852048 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 9 when performing action Right is 2.49.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.02085696  0.01969441 -0.09998142 -0.00631821]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49134383  0.781513   -0.02400536  0.00171039]\n",
            "Evaluating reward...\n",
            "In state 13, action Down is better than Right because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "7\n",
            "3\n",
            "-0.4666558242291819\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.49852048 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 8 when performing action Right is -0.47.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.02085696  0.01969441 -0.11953649 -0.00631821]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-4.89013562e-01  8.03327928e-01 -2.40053562e-02 -6.03298998e-04]\n",
            "Evaluating reward...\n",
            "In state 11, action Up is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "3\n",
            "-0.03848194588318974\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.49852048 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Right is -0.04.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00129355  0.01969441 -0.15757694 -0.00631821]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-4.89013562e-01  8.21217346e-01 -2.40053562e-02 -6.03298998e-04]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "3\n",
            "-0.061104891098309966\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.49852048 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action Right is -0.06.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00657022  0.01969441 -0.18467086 -0.00631821]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-4.89013562e-01  8.37640010e-01 -2.40053562e-02 -6.03415352e-04]\n",
            "Evaluating reward...\n",
            "In state 14, action Down is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "1\n",
            "-1.16720719900216\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.23344144  0.49852048  0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action Down is -1.17.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00657022  0.01969441 -0.22286829 -0.00631821]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-4.89013562e-01  8.52788890e-01 -2.40053562e-02 -6.03415352e-04]\n",
            "Evaluating reward...\n",
            "In state 14, action Down is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "4\n",
            "0.12800342114124216\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.23344144  0.49852048  0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action No-Op is 0.13.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00609203  0.01969441 -0.26026826 -0.00631821]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-4.89011126e-01  8.66847929e-01 -2.40053562e-02 -6.03415352e-04]\n",
            "Evaluating reward...\n",
            "In state 5, action Left is better than Up because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "4\n",
            "0.413550893172105\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.23344144  0.49852048  0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action No-Op is 0.41.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02565544  0.01969441 -0.29830871 -0.00631821]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-4.89021263e-01  8.79972739e-01 -2.40053562e-02 -8.01367752e-04]\n",
            "Evaluating reward...\n",
            "In state 11, action Down is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "4\n",
            "-0.1993345058448608\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action No-Op is -0.20.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[0.         0.         0.00056809 0.        ]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00122176  0.00271747  0.50475816 -0.00578614]\n",
            "Evaluating reward...\n",
            "In state 18, action Left is better than No-Op because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "4\n",
            "-1.7495340919182218\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action No-Op is -1.75.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.25384954e-05  0.00000000e+00  1.54875473e-03 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00113903  0.00271747  0.57916393 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 14, action Down is better than Up because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "2\n",
            "-0.035188770467189365\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Left is -0.04.\n",
            "\n",
            "Sampling demo... \n",
            "15\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.25384954e-05  0.00000000e+00  2.43094526e-03 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 16 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00113902  0.00271747  0.6321191  -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 19, action Left is better than Down because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "2\n",
            "0.1555066016687183\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action Left is 0.16.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.95042720e-03 -2.06399888e-03  7.19419060e-03 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00114483  0.00271747  0.67326324 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 5, action Left is better than No-Op because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "0\n",
            "0.2107591428923677\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Up is 0.21.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.93928843e-03 -2.06399888e-03  8.06538124e-03 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00114482  0.00271747  0.70668044 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is better than Down because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "0\n",
            "-0.36207875970442954\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Up is -0.36.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.93928843e-03 -2.06399888e-03  8.94757177e-03 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00114896  0.00271747  0.73539039 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 5, action Left is better than No-Op because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "2\n",
            "0.07288012070239688\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Left is 0.07.\n",
            "\n",
            "Sampling demo... \n",
            "15\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.93928843e-03 -2.06399888e-03  9.82976230e-03 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 16 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00114925  0.00271747  0.76009644 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 1, action Right is better than Down because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "0\n",
            "-0.1324427909920644\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Up is -0.13.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-3.90225413e-03 -4.12799776e-03  1.45930076e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50034015  0.01230116  0.75109049 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is worse than Left because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "17\n",
            "0\n",
            "0.4881148325764807\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 18 when performing action Up is 0.49.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-3.89111536e-03 -4.12799776e-03  1.54641983e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50033672  0.01230116  0.77421696 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than Up because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "0\n",
            "-0.2534222458260766\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Up is -0.25.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.25241135e-02 -1.07414447e-02  2.16754902e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50033353  0.01230116  0.79487874 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 5, action Left is better than Right because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "1\n",
            "-0.26701818434050456\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Down is -0.27.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.25115750e-02 -1.07414447e-02  2.26561568e-02 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50033073  0.01230116  0.81354347 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is better than Down because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "7\n",
            "0\n",
            "-0.11570724258922364\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 8 when performing action Up is -0.12.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.25115750e-02 -1.07414447e-02  2.35791547e-02 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50033073  0.01230116  0.83056792 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 18, action Left is better than No-Op because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "14\n",
            "3\n",
            "0.14721616537677734\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 15 when performing action Right is 0.15.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.02385209 -0.01074144  0.02713716 -0.00148454]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50033073  0.01230116  0.84623875 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 14, action Down is better than Right because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "10\n",
            "3\n",
            "2.3429357576533785\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.          0.46858715]\n",
            "Evaluating reward...\n",
            "The reward in state 11 when performing action Right is 2.34.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.02385209 -0.01074144  0.02770525 -0.00148454]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50033073  0.01230116  0.86074649 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 12, action Down is better than No-Op because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "3\n",
            "2.7496820422828034\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.34990682  0.          0.54993641  0.46858715]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Right is 2.75.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.02384096 -0.01074144  0.02857644 -0.00148454]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50033073  0.01230116  0.87423082 -0.00587308]\n",
            "Evaluating reward...\n",
            "In state 7, action Down is better than Right because it may eventually lead you through state 10 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "1\n",
            "-0.12931991416175262\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action Down is -0.13.\n",
            "\n",
            "Sampling demo... \n",
            "5\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.27184610e-04 -5.62161958e-05 -1.77686693e-02  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 6 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00709208  0.00470859 -0.00381432 -0.50170855]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is worse than Up because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "2\n",
            "-0.01637673094310185\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Left is -0.02.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-6.99406392e-04 -5.62161958e-05 -5.47023006e-02  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00709491 -0.5053252  -0.00381626 -0.50170855]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is worse than Right because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "0\n",
            "-0.19522561657853338\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Up is -0.20.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-9.31464085e-04 -5.62161958e-05 -7.28521057e-02  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50266946 -0.50532761  0.000994   -0.501711  ]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is worse than Down because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "1\n",
            "0.020841775628261595\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Down is 0.02.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-9.31464085e-04 -5.62161958e-05 -9.17883740e-02  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50267244 -0.50532733  0.00797312 -0.57728617]\n",
            "Evaluating reward...\n",
            "In state 14, action Up is worse than Left because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "2\n",
            "0.38076144535615514\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Left is 0.38.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00289443 -0.00212022 -0.10580895  0.        ]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50274004 -0.57763508  0.00570489 -0.57728617]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is worse than Right because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "3\n",
            "-1.8393832609073313\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action Right is -1.84.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00289443 -0.00212022 -0.12390695  0.        ]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50274611 -0.6290392   0.00989812 -0.57728615]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is worse than Right because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "0\n",
            "-0.6900899970792718\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Up is -0.69.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00289443 -0.00212022 -0.14200495  0.        ]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.5762491  -0.62904302  0.00816175 -0.57728992]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is worse than Right because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "4\n",
            "-0.20290689327754854\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action No-Op is -0.20.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00288329 -0.00212022 -0.14113376  0.        ]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.57254596 -0.67414121  0.01168802 -0.57728992]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is worse than No-Op because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "4\n",
            "-0.16011857997555584\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action No-Op is -0.16.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00311828 -0.00212022 -0.15951273  0.        ]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.56831863 -0.67414122  0.01988434 -0.63441893]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is worse than Up because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "3\n",
            "0.3940634472802427\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Right is 0.39.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00311828 -0.00212022 -0.15858973  0.        ]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.62346535 -0.67414148  0.01501985 -0.6344189 ]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is worse than Down because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "3\n",
            "-0.17169732493996426\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Right is -0.17.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-3.34590432e-03 -2.12021508e-03 -1.76392891e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.66334418 -0.674144    0.01190044 -0.63442133]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is worse than Right because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "0\n",
            "-0.0008262311513669649\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Up is -0.00.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-3.53311402e-03 -2.12021508e-03 -1.94928986e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.6983841  -0.67414399  0.00715155 -0.6344215 ]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is worse than Up because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "3\n",
            "0.25159790133253407\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Right is 0.25.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-3.53311402e-03 -2.12021508e-03 -2.13026984e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.69838979 -0.67414397  0.00884048 -0.67237868]\n",
            "Evaluating reward...\n",
            "In state 14, action Up is worse than Right because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "4\n",
            "-0.2378134895747599\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665  0.         -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action No-Op is -0.24.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-4.01129820e-03 -2.12021508e-03 -2.50426949e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.73266573 -0.67414396 -0.36294422 -0.67237884]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is worse than Up because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "8\n",
            "2\n",
            "-1.6058764961414407\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.36787665 -0.3211753  -0.138018    0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 9 when performing action Left is -1.61.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 3.85246806e-03 -2.12021508e-03 -2.77520872e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.75772014 -0.67414405 -0.36295374 -0.67237892]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is worse than No-Op because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "3\n",
            "-0.08354339412398208\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Right is -0.08.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0001704   0.         -0.01393941  0.01484193]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00098232 -0.00978061  0.0013618   0.50209251]\n",
            "Evaluating reward...\n",
            "In state 12, action Down is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "2\n",
            "0.2110870762995306\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.04221742 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Left is 0.21.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0001704   0.         -0.05082123  0.01484193]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00098232 -0.00978061  0.0013618   0.57740333]\n",
            "Evaluating reward...\n",
            "In state 19, action Up is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "3\n",
            "-0.024115873201679534\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.04221742 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Right is -0.02.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00029439  0.         -0.08717401  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00098232 -0.00978061  0.0013618   0.63030333]\n",
            "Evaluating reward...\n",
            "In state 17, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "8\n",
            "0\n",
            "-0.09968711348305745\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.         -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 9 when performing action Up is -0.10.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01714891  0.         -0.12094944  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00097986 -0.00978061  0.01097253  0.66211318]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "3\n",
            "-0.028280283945860685\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.         -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Right is -0.03.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01667669  0.         -0.15788307  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00097986 -0.00978061  0.01097253  0.69775313]\n",
            "Evaluating reward...\n",
            "In state 15, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "4\n",
            "-0.1651503680231721\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.         -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action No-Op is -0.17.\n",
            "\n",
            "Sampling demo... \n",
            "15\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01667669  0.         -0.15700088  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 16 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-5.43544332e-04 -1.01749203e-02  1.09725330e-02  7.27751133e-01]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is better than Right because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "1\n",
            "0.027740877462900253\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.         -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Down is 0.03.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01684709  0.         -0.17094029  0.06773807]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-5.43544332e-04 -1.01749203e-02  1.09725330e-02  7.53647849e-01]\n",
            "Evaluating reward...\n",
            "In state 19, action Up is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "0\n",
            "-0.13300401407949075\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.         -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Up is -0.13.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0342904   0.         -0.20471573  0.08607918]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-5.43544332e-04 -1.01749203e-02  1.09725330e-02  7.76485094e-01]\n",
            "Evaluating reward...\n",
            "In state 17, action Right is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "1\n",
            "-0.539649871657623\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.         -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Down is -0.54.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03467839 -0.00661345 -0.19850444  0.08607918]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-5.43544332e-04 -1.01749203e-02  1.09725330e-02  7.96892540e-01]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "1\n",
            "0.6549132463224364\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.13098265 -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action Down is 0.65.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03420617 -0.00661345 -0.23543807  0.08607918]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-2.40755957e-04 -1.04293849e-02  1.09725330e-02  8.15390298e-01]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "14\n",
            "3\n",
            "-0.1451103803170105\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.13098265 -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 15 when performing action Right is -0.15.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03437657 -0.00661345 -0.24937748  0.10092111]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-2.38331173e-04 -1.04293849e-02  2.08583501e-02  8.22567030e-01]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "13\n",
            "3\n",
            "0.0827639408939169\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.13098265 -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 14 when performing action Right is 0.08.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03438771 -0.00661345 -0.24850629  0.10092111]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-2.38331173e-04 -1.04293849e-02  2.08583501e-02  8.38852823e-01]\n",
            "Evaluating reward...\n",
            "In state 4, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "0\n",
            "-0.23415643782390855\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.13098265 -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Up is -0.23.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03438771 -0.00661345 -0.24831074  0.10092111]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-2.38331173e-04 -1.04293849e-02  2.08583501e-02  8.53884683e-01]\n",
            "Evaluating reward...\n",
            "In state 4, action Right is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "13\n",
            "3\n",
            "-0.0838543335205089\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.13098265 -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 14 when performing action Right is -0.08.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03438771 -0.00661345 -0.28519256  0.10092111]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-2.38331173e-04 -1.04293849e-02  2.08583501e-02  8.67837573e-01]\n",
            "Evaluating reward...\n",
            "In state 5, action Down is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "1\n",
            "0.48166993178869244\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.13098265 -0.01993742  0.04221742  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Down is 0.48.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03391549 -0.00661345 -0.32212619  0.10092111]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-2.36238983e-04 -1.04293849e-02  2.08583501e-02  8.80880442e-01]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "3\n",
            "0.12323672482276675\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Right is 0.12.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00023206  0.         -0.01814981  0.        ]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 2.52839360e-03 -5.09322915e-01  3.22084972e-04  2.04209283e-04]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is worse than Right because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "0\n",
            "0.378883156930247\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Up is 0.38.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-4.59683286e-04  0.00000000e+00 -3.59529647e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.11600710e-03 -5.09322711e-01 -5.04562335e-01  1.97670791e-04]\n",
            "Evaluating reward...\n",
            "In state 7, action Down is worse than Right because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "0\n",
            "-0.7328886709881163\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.14657773  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action Up is -0.73.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-6.87308879e-04  0.00000000e+00 -5.37561242e-02 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01147109 -0.50932269 -0.50456227 -0.50442187]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is worse than Left because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "0\n",
            "-0.47476846994153654\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.21221588  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action Up is -0.47.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-9.19366572e-04  0.00000000e+00 -7.19059294e-02 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.0089258  -0.50932266 -0.50461774 -0.57666181]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is worse than Left because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "4\n",
            "-0.6430339635012028\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.2983795  0.         0.         0.       ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action No-Op is -0.64.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.15435429e-03  0.00000000e+00 -9.02848987e-02 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.50652459 -0.50932264 -0.50461947 -0.57666213]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is worse than No-Op because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "4\n",
            "-0.4633736981098127\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.33137834  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action No-Op is -0.46.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.38641198e-03  0.00000000e+00 -1.08434704e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.58001943 -0.50932351 -0.50462455 -0.57692508]\n",
            "Evaluating reward...\n",
            "In state 14, action Up is worse than Down because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "1\n",
            "-0.21074125595819612\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.33137834  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Down is -0.21.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.57362168e-03  0.00000000e+00 -1.26970798e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.58003876 -0.5093223  -0.57920623 -0.57692408]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is worse than Left because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "4\n",
            "0.0819001329724318\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.33137834  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action No-Op is 0.08.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.57362168e-03  0.00000000e+00 -1.46525871e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.63207547 -0.50932298 -0.57921019 -0.57718676]\n",
            "Evaluating reward...\n",
            "In state 14, action Up is worse than Left because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "1\n",
            "-0.23864483127070932\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.33137834  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Down is -0.24.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-1.57362168e-03  0.00000000e+00 -1.64904841e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.67293713 -0.50932217 -0.57921597 -0.57718753]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is worse than Up because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "2\n",
            "0\n",
            "-0.567859265035853\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.37867452  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 3 when performing action Up is -0.57.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.04584346e-03  0.00000000e+00 -2.01838472e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.70646708 -0.50932295 -0.57922123 -0.57718684]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is worse than Right because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "1\n",
            "-0.3506561309117323\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.37867452  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Down is -0.35.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.51806524e-03  0.00000000e+00 -2.38772103e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.70646661 -0.50932302 -0.57922114 -0.63029164]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is worse than Right because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "2\n",
            "-0.1060961039083842\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.37867452  0.          0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Left is -0.11.\n",
            "\n",
            "Sampling demo... \n",
            "5\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.74524985e-03 -5.62161958e-05 -2.56540772e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 6 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.73493833 -0.50932222 -0.57922591 -0.63029208]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is worse than Down because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "8\n",
            "4\n",
            "-1.4714569257022854\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.37867452 -0.29429139  0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 9 when performing action No-Op is -1.47.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 5.11851640e-03 -5.62161958e-05 -2.83634695e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.75970228 -0.50932281 -0.5792304  -0.63029149]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is worse than Up because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "4\n",
            "-0.4149635831366688\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.37867452 -0.29429139  0.          0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action No-Op is -0.41.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 5.11851640e-03 -5.62161958e-05 -3.03189768e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.7597019  -0.50932287 -0.57923033 -0.6714671 ]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is worse than No-Op because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "2\n",
            "-1.7762198559138371\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[-0.37867452 -0.29429139 -0.35524397  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Left is -1.78.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 4.88352869e-03 -5.62161958e-05 -3.21568737e-01 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.78163695 -0.50932329 -0.57923344 -0.67146735]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is worse than No-Op because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "0\n",
            "-0.08981923336454942\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Up is -0.09.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.25384954e-05  0.00000000e+00  9.80666684e-04 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00631991  0.50801862 -0.00294827 -0.00217718]\n",
            "Evaluating reward...\n",
            "In state 2, action Left is better than Up because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "10\n",
            "1\n",
            "-1.9720007091071314\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.39440014]\n",
            "Evaluating reward...\n",
            "The reward in state 11 when performing action Down is -1.97.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 2.50769909e-05  0.00000000e+00  1.96133337e-03 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00630822  0.58196552 -0.00294827 -0.00217814]\n",
            "Evaluating reward...\n",
            "In state 8, action Up is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "3\n",
            "-0.35643191836423305\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.39440014]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Right is -0.36.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.68459375e-02  1.76866246e-02 -3.06088615e-02 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00588039  0.5821361  -0.49964721 -0.00217814]\n",
            "Evaluating reward...\n",
            "In state 7, action Down is worse than No-Op because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "10\n",
            "3\n",
            "-2.09657477148835\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 11 when performing action Right is -2.10.\n",
            "\n",
            "Sampling demo... \n",
            "12\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.68459375e-02  1.76866246e-02 -6.90133359e-02 -7.50261099e-05]\n",
            "Evaluating reward...\n",
            "In state 13 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00588039  0.63433928 -0.49964721 -0.00217814]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "2\n",
            "-0.03151239465342289\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Left is -0.03.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01725351  0.01768662 -0.06440313 -0.00498372]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00588039  0.67505895 -0.49964721 -0.00217814]\n",
            "Evaluating reward...\n",
            "In state 18, action Left is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "14\n",
            "1\n",
            "-0.3400101521688162\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 15 when performing action Down is -0.34.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03681692  0.01768662 -0.10244358 -0.00498372]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00587489  0.70855137 -0.49964721 -0.00217836]\n",
            "Evaluating reward...\n",
            "In state 8, action Up is better than Right because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "3\n",
            "0.05501268277832614\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Right is 0.06.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03681692  0.01768662 -0.10152058 -0.00498372]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00587489  0.73699824 -0.49964721 -0.00217836]\n",
            "Evaluating reward...\n",
            "In state 17, action Left is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "2\n",
            "0.19031507207943754\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Left is 0.19.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0368282   0.01768662 -0.10063839 -0.00498372]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.005931    0.761798   -0.49964721 -0.00252139]\n",
            "Evaluating reward...\n",
            "In state 11, action Down is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "3\n",
            "-0.06619908760323888\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Right is -0.07.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03548768  0.01768662 -0.09708039 -0.00639324]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.0059213   0.78351793 -0.49964721 -0.00253127]\n",
            "Evaluating reward...\n",
            "In state 8, action Up is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "2\n",
            "-0.10791097338484719\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Left is -0.11.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03549882  0.01768662 -0.0962092  -0.00639324]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00591823  0.80326062 -0.49964721 -0.00253127]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "8\n",
            "0\n",
            "1.4947596580081488\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.29895193  0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 9 when performing action Up is 1.49.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03564965  0.0308238  -0.10854752 -0.00639324]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00609095  0.80284182 -0.5754101  -0.00253127]\n",
            "Evaluating reward...\n",
            "In state 7, action Down is worse than No-Op because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "3\n",
            "0.18093499362006457\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.29895193  0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Right is 0.18.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03517743  0.0308238  -0.14548115 -0.00639324]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00609057  0.82058864 -0.57541043 -0.00253127]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "2\n",
            "0.15521159211433863\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.29895193  0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Left is 0.16.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03518871  0.0308238  -0.14459896 -0.00639324]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00609057  0.8370226  -0.57541043 -0.00253127]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "3\n",
            "-0.18302653797663332\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.29895193  0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Right is -0.18.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03520124  0.0308238  -0.14361829 -0.00643075]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00609375  0.85202851 -0.57541043 -0.00253127]\n",
            "Evaluating reward...\n",
            "In state 2, action Left is better than Right because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "7\n",
            "2\n",
            "-0.20544470843518825\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.29895193  0.         -0.73483507]\n",
            "Evaluating reward...\n",
            "The reward in state 8 when performing action Left is -0.21.\n",
            "\n",
            "Sampling demo... \n",
            "12\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03520124  0.0308238  -0.18202277 -0.00643075]\n",
            "Evaluating reward...\n",
            "In state 13 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00627558  0.85159826 -0.62848199 -0.00253127]\n",
            "Evaluating reward...\n",
            "In state 7, action Down is worse than No-Op because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "3\n",
            "-0.6765401274636609\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Right is -0.68.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.25384954e-05  0.00000000e+00  9.80666684e-04 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00660526  0.00127581  0.00179038 -0.51790074]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is worse than Up because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "3\n",
            "-0.12259839656788858\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Right is -0.12.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.25384954e-05  0.00000000e+00  1.90366458e-03 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01150514  0.49981727  0.0017298  -0.517909  ]\n",
            "Evaluating reward...\n",
            "In state 8, action Up is better than Right because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "4\n",
            "0.21698870028698553\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action No-Op is 0.22.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-4.59683286e-04  0.00000000e+00 -3.50299668e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01151623  0.57661882  0.0017298  -0.5179091 ]\n",
            "Evaluating reward...\n",
            "In state 8, action Up is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "1\n",
            "-0.01988902563602502\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Down is -0.02.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-9.37867464e-04  0.00000000e+00 -7.24299319e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01151623  0.63019189  0.0017298  -0.5179091 ]\n",
            "Evaluating reward...\n",
            "In state 15, action Up is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "0\n",
            "-0.11948867085917617\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Up is -0.12.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.58829931e-02  1.76866246e-02 -1.05000127e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01152257  0.63067562  0.0017298  -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 5, action Down is worse than No-Op because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "2\n",
            "0.023960208228924817\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Left is 0.02.\n",
            "\n",
            "Sampling demo... \n",
            "12\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.58829931e-02  1.76866246e-02 -1.43404601e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 13 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01152257  0.67170824  0.0017298  -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "4\n",
            "0.3181719072139275\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action No-Op is 0.32.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.58829931e-02  1.76866246e-02 -1.42481603e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01152257  0.7056957   0.0017298  -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 1, action Down is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "14\n",
            "4\n",
            "-0.22420637909833557\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 15 when performing action No-Op is -0.22.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.54048089e-02  1.76866246e-02 -1.79881568e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01150071  0.73424508  0.0017298  -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 3, action Left is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "4\n",
            "-0.16224818706152858\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action No-Op is -0.16.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.54048089e-02  1.76866246e-02 -2.18078996e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01150071  0.75908675  0.0017298  -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 17, action Left is better than Up because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "0\n",
            "-0.17358917971851956\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Up is -0.17.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.49266247e-02  1.76866246e-02 -2.55478962e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01156131  0.78134196  0.00143827 -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 10, action Up is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "10\n",
            "3\n",
            "-0.055154957020226034\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.01103099]\n",
            "Evaluating reward...\n",
            "The reward in state 11 when performing action Right is -0.06.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.49377635e-02  1.76866246e-02 -2.54607771e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01156131  0.80127293  0.00143827 -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 1, action Down is better than Left because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "3\n",
            "-0.07789179499393467\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.01103099]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Right is -0.08.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.50885916e-02  3.08238011e-02 -2.66946093e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01604308  0.79219724 -0.50025815 -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 3, action Down is worse than Left because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "1\n",
            "0.01862952717280782\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.01103099]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Down is 0.02.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 3.46520054e-02  3.08238011e-02 -3.04986544e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01603103  0.8109049  -0.50025815 -0.58772339]\n",
            "Evaluating reward...\n",
            "In state 3, action Left is better than Down because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "3\n",
            "0.133934111774124\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.01103099]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Right is 0.13.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 4.25157716e-02  3.08238011e-02 -3.32080467e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01603068  0.82796307 -0.50025815 -0.58772376]\n",
            "Evaluating reward...\n",
            "In state 14, action Down is better than Up because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "4\n",
            "-0.05370046490015905\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.          0.         -0.01103099]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action No-Op is -0.05.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02390235  0.0308238  -0.32747026 -0.00494621]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.01603068  0.84381186 -0.50025815 -0.58772376]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is better than No-Op because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "1\n",
            "0.054951489850621725\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Down is 0.05.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.0004648   0.         -0.03635277  0.01971311]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00452891 -0.00174643 -0.01344285  0.5080177 ]\n",
            "Evaluating reward...\n",
            "In state 17, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "3\n",
            "-0.07682215885093827\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action Right is -0.08.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.0004648   0.         -0.05590785  0.01971311]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00462767 -0.49940779 -0.01344285  0.50779305]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is worse than Up because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "2\n",
            "0.05815619293324879\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Left is 0.06.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.0004648   0.         -0.09278967  0.01971311]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00454485 -0.49940779 -0.01352561  0.58115471]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "0\n",
            "0.04371655729327744\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Up is 0.04.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00029439  0.         -0.10672908  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00942256 -0.49940779 -0.01353651  0.63303953]\n",
            "Evaluating reward...\n",
            "In state 10, action Down is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "4\n",
            "-0.20808027697751613\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action No-Op is -0.21.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00028312  0.         -0.10584689  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00942256 -0.49940779 -0.01353651  0.67359954]\n",
            "Evaluating reward...\n",
            "In state 17, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "4\n",
            "0.10711475183380928\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action No-Op is 0.11.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00028312  0.         -0.12540196  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00942256 -0.49940779 -0.01353651  0.70695575]\n",
            "Evaluating reward...\n",
            "In state 17, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "3\n",
            "0.2518426265789522\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Right is 0.25.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00028312  0.         -0.16280193  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00942267 -0.49940779 -0.01353651  0.7356178 ]\n",
            "Evaluating reward...\n",
            "In state 4, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "3\n",
            "-0.09522167414480591\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Right is -0.10.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00028312  0.         -0.20099936  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00942267 -0.49940779 -0.01353651  0.76053768]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "1\n",
            "-0.10218243408544585\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Down is -0.10.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00028312  0.         -0.23788118  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00942267 -0.49940779 -0.01353651  0.78264649]\n",
            "Evaluating reward...\n",
            "In state 12, action Down is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "4\n",
            "-0.14048752188603597\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action No-Op is -0.14.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01716019  0.         -0.27165661  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00943408 -0.49940779 -0.01353651  0.80224962]\n",
            "Evaluating reward...\n",
            "In state 3, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "1\n",
            "-0.03438345075977897\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Down is -0.03.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01668201  0.         -0.30905658  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00943408 -0.49940779 -0.01353663  0.82025988]\n",
            "Evaluating reward...\n",
            "In state 13, action Down is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "4\n",
            "0.1309942918949239\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action No-Op is 0.13.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01668201  0.         -0.34645654  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49763749 -0.49940779 -0.01349026  0.81999099]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is worse than Left because it may eventually lead you through state 3 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "2\n",
            "0.16338912569213898\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Left is 0.16.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01620382  0.         -0.38385651  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49763749 -0.49940827 -0.01349026  0.83630414]\n",
            "Evaluating reward...\n",
            "In state 12, action Down is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "13\n",
            "3\n",
            "-0.036350445200100764\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 14 when performing action Right is -0.04.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03576724  0.         -0.42189696  0.05289615]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49775735 -0.57534238 -0.01349026  0.83606959]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is worse than Up because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "14\n",
            "4\n",
            "0.2335187214811608\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 15 when performing action No-Op is 0.23.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03530244  0.         -0.45824973  0.07260926]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.49774853 -0.62866142 -0.01349026  0.83595348]\n",
            "Evaluating reward...\n",
            "In state 6, action Down is worse than Left because it may eventually lead you through state 9 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "7\n",
            "3\n",
            "-0.22686359880371407\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 8 when performing action Right is -0.23.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[1.12794103e-05 0.00000000e+00 8.82190528e-04 0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 5.03508610e-01 -1.18218266e-02  5.54722757e-04  2.96009362e-04]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "1\n",
            "0.18067519885939734\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Down is 0.18.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-2.16346182e-04  0.00000000e+00 -1.69209690e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 5.78958036e-01 -1.18218266e-02  5.54667394e-04  2.96009362e-04]\n",
            "Evaluating reward...\n",
            "In state 5, action Left is better than Right because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "8\n",
            "0\n",
            "0.617991855154702\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 9 when performing action Up is 0.62.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.71651461e-04 -6.61344691e-03 -1.07096771e-02 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 6.31957459e-01 -1.18218266e-02  5.54650939e-04  2.95589646e-04]\n",
            "Evaluating reward...\n",
            "In state 8, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "0\n",
            "0.16128108569395216\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Up is 0.16.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00057922 -0.00661345 -0.00609947 -0.00494621]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 6.73148361e-01 -1.18154566e-02  5.54334889e-04  2.95589646e-04]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than Right because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "2\n",
            "-0.057043695827792174\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Left is -0.06.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00868925 -0.00661345 -0.01393224 -0.00494621]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.06923995e-01 -1.18154566e-02  5.54331417e-04 -1.05208154e-04]\n",
            "Evaluating reward...\n",
            "In state 11, action Up is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "3\n",
            "-0.04351453012933097\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Right is -0.04.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00870039 -0.00661345 -0.01306104 -0.00494621]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.35592866e-01 -1.18154566e-02  5.58307624e-04 -1.05208154e-04]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "2\n",
            "0.13302300493042102\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Left is 0.13.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02552125  0.01107318 -0.04563124 -0.00494621]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.60276655e-01 -1.18154566e-02  5.58305107e-04 -1.24907778e-04]\n",
            "Evaluating reward...\n",
            "In state 11, action Up is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "7\n",
            "2\n",
            "0.07073853213453349\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 8 when performing action Left is 0.07.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.04296456  0.01107318 -0.07940668  0.0133949 ]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.82372920e-01 -1.18154566e-02  5.58302876e-04 -1.24907778e-04]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "0\n",
            "-0.1860609341930693\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.         0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Up is -0.19.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.06040787  0.01107318 -0.11318211  0.03173601]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.02228231e-01 -1.18154566e-02  5.58192835e-04 -1.21731210e-04]\n",
            "Evaluating reward...\n",
            "In state 14, action Up is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "2\n",
            "0.22702635828485396\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.04540527 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Left is 0.23.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.06041901  0.01107318 -0.11231092  0.03173601]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.20222629e-01 -1.20239834e-02  5.58192835e-04 -1.21731210e-04]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "4\n",
            "-0.18722315043540264\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.04540527 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action No-Op is -0.19.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.06041901  0.01107318 -0.14919274  0.03173601]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.36688735e-01 -1.20239834e-02  5.58192835e-04 -1.21731210e-04]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "0\n",
            "-0.22813283177744265\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.04540527 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Up is -0.23.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.06043029  0.01107318 -0.14831055  0.03173601]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.51720455e-01 -1.20270807e-02  5.58192835e-04 -1.21731210e-04]\n",
            "Evaluating reward...\n",
            "In state 6, action Up is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "0\n",
            "0.050387952596917404\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.04540527 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Up is 0.05.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.07787359  0.01107318 -0.18208599  0.05007711]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.65678283e-01 -1.20270807e-02  5.58192835e-04 -1.21731210e-04]\n",
            "Evaluating reward...\n",
            "In state 1, action Right is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "3\n",
            "0.012822867567815945\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.04540527 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Right is 0.01.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.07787359  0.01107318 -0.1812038   0.05007711]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.78840627e-01 -1.20270807e-02  5.58192835e-04 -1.21731210e-04]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "0\n",
            "0.014756137250214916\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.12359837 0.04540527 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action Up is 0.01.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0953169   0.01107318 -0.21497923  0.06841822]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.91180927e-01 -1.21725614e-02  5.58192835e-04 -1.21731210e-04]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is better than Right because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "0\n",
            "-0.15804750311229082\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Up is -0.16.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00811003  0.         -0.00783276  0.        ]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 5.02373472e-01  1.18428950e-02 -4.87068891e-04 -4.57800252e-04]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "0\n",
            "0.0943977203314632\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Up is 0.09.\n",
            "\n",
            "Sampling demo... \n",
            "6\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02767345  0.         -0.04587321  0.        ]\n",
            "Evaluating reward...\n",
            "In state 7 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 5.77608224e-01  1.18428950e-02 -4.87124750e-04 -5.69237973e-04]\n",
            "Evaluating reward...\n",
            "In state 11, action Up is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "3\n",
            "-0.22830361674506644\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Right is -0.23.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02767345  0.         -0.06542829  0.        ]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 6.30377836e-01  1.66265673e-02 -5.15238758e-03 -5.69237974e-04]\n",
            "Evaluating reward...\n",
            "In state 15, action Up is better than Right because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "4\n",
            "-0.1093331919830269\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action No-Op is -0.11.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02767345  0.         -0.0648602   0.        ]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 6.71411348e-01  1.66265673e-02 -2.80458403e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 19, action Left is better than Up because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "2\n",
            "-0.413483413757363\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Left is -0.41.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02767345  0.         -0.08441527  0.        ]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.05450436e-01  1.66265673e-02 -2.80461876e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 5, action Left is better than Right because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "2\n",
            "-0.3101045698199078\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action Left is -0.31.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02767345  0.         -0.10251327  0.        ]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.34318597e-01  1.66265673e-02 -2.80701401e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "1\n",
            "-0.15926167547111333\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Down is -0.16.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02767345  0.         -0.10159027  0.        ]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.59401894e-01  1.66265673e-02 -2.80704007e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 1, action Right is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "14\n",
            "1\n",
            "-0.1465603770502007\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 15 when performing action Down is -0.15.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02768459  0.         -0.10071908  0.        ]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.81376260e-01  1.66267809e-02 -2.80706326e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "2\n",
            "-0.17812490385050195\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action Left is -0.18.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02807258 -0.00661345 -0.09450779  0.        ]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.01303441e-01  1.66267809e-02 -2.80706326e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "3\n",
            "0\n",
            "0.035945764918287416\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 4 when performing action Up is 0.04.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.02822341  0.00652373 -0.10684611  0.        ]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.19409475e-01  1.63865402e-02 -2.80706326e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "1\n",
            "0.0769910134903494\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Down is 0.08.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 2.82359502e-02  6.52372961e-03 -1.05865444e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.35778046e-01  1.63865402e-02 -2.80706326e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 4, action Left is better than Right because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "0\n",
            "-0.11600870835210705\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action Up is -0.12.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 2.82472296e-02  6.52372961e-03 -1.04983254e-01 -3.75130550e-05]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.51030759e-01  1.63865402e-02 -2.80706326e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "0\n",
            "0.1121279325001296\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Up is 0.11.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.04569054  0.00652373 -0.13875869  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.51201062e-01  4.95922424e-01 -2.80706326e-04 -5.56504076e-03]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than Right because it may eventually lead you through state 9 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "3\n",
            "0.043984526169383725\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Right is 0.04.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.06313384  0.00652373 -0.17253412  0.0366447 ]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.65195810e-01  4.95922424e-01 -2.80706326e-04 -5.57411944e-03]\n",
            "Evaluating reward...\n",
            "In state 11, action Up is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "7\n",
            "3\n",
            "0.008931262852802043\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 8 when performing action Right is 0.01.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.06313384  0.00652373 -0.20941595  0.0366447 ]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.64744233e-01  4.95922424e-01 -2.88872019e-04 -4.98599902e-01]\n",
            "Evaluating reward...\n",
            "In state 18, action Right is worse than Left because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "0\n",
            "0.25887116915828706\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Up is 0.26.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.          0.         -0.01955507  0.        ]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 2.52839360e-03  2.04209283e-04  3.22084972e-04 -5.09322915e-01]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is worse than No-Op because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "1\n",
            "-1.5594656868461327\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.31189314  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Down is -1.56.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01682086  0.01768662 -0.05212527  0.        ]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 5.04944003e-01  1.93616916e-04 -4.97883479e-03 -5.09324361e-01]\n",
            "Evaluating reward...\n",
            "In state 10, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "1\n",
            "-1.3258949911510725\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Down is -1.33.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01683214  0.01768662 -0.05124308  0.        ]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 5.79931617e-01  2.04108930e-04 -4.97888967e-03 -5.09324361e-01]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "1\n",
            "-0.18665438916335075\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Down is -0.19.\n",
            "\n",
            "Sampling demo... \n",
            "13\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03427545  0.01768662 -0.08501851  0.01834111]\n",
            "Evaluating reward...\n",
            "In state 14 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 6.32711008e-01  2.11702123e-04 -4.97890604e-03 -5.09324361e-01]\n",
            "Evaluating reward...\n",
            "In state 12, action Up is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "2\n",
            "-0.16784433058643372\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Left is -0.17.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03428798  0.01768662 -0.08403785  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 6.24347403e-01  2.11702123e-04 -4.94481811e-01 -5.09324361e-01]\n",
            "Evaluating reward...\n",
            "In state 7, action Down is worse than Right because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "0\n",
            "0.039388394247292194\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Up is 0.04.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03429926  0.01768662 -0.08315566  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 6.67065209e-01  2.11702123e-04 -4.94475548e-01 -5.09324361e-01]\n",
            "Evaluating reward...\n",
            "In state 17, action Up is better than Down because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "2\n",
            "0.12577002560564962\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action Left is 0.13.\n",
            "\n",
            "Sampling demo... \n",
            "15\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03429926  0.01768662 -0.08227346  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 16 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.01859727e-01  7.62566769e-06 -4.94271302e-01 -5.09324361e-01]\n",
            "Evaluating reward...\n",
            "In state 16, action Right is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "0\n",
            "-0.2021093756585312\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action Up is -0.20.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03429926  0.01768662 -0.08139127  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.31235103e-01  7.62566769e-06 -4.94271302e-01 -5.09324361e-01]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than Up because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "0\n",
            "-0.10931461065626509\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Up is -0.11.\n",
            "\n",
            "Sampling demo... \n",
            "18\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03429926  0.01768662 -0.08046828  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 19 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.73124497  0.00074208 -0.57287759 -0.50932436]\n",
            "Evaluating reward...\n",
            "In state 15, action Right is worse than Up because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "1\n",
            "0.014402358424976617\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Down is 0.01.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03429926  0.01768662 -0.07990019  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.73087935  0.00074208 -0.57307727 -0.58177757]\n",
            "Evaluating reward...\n",
            "In state 18, action Right is worse than Left because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "3\n",
            "0.08313713925954183\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Right is 0.08.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0343104   0.01768662 -0.079029    0.01830359]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.56125504e-01  7.42083053e-04 -5.73077268e-01 -5.81795562e-01]\n",
            "Evaluating reward...\n",
            "In state 11, action Up is better than No-Op because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "1\n",
            "-0.04508075081689661\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action Down is -0.05.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03432168  0.01768662 -0.07814681  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.78708688e-01  7.52555262e-04 -5.73084015e-01 -5.81795562e-01]\n",
            "Evaluating reward...\n",
            "In state 15, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "13\n",
            "3\n",
            "0.15701703321980937\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 14 when performing action Right is 0.16.\n",
            "\n",
            "Sampling demo... \n",
            "8\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03470968  0.01107318 -0.07193552  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 9 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.98691470e-01  7.52555262e-04 -5.73097449e-01 -5.81795562e-01]\n",
            "Evaluating reward...\n",
            "In state 10, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "1\n",
            "-0.08008083679200968\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Down is -0.08.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03470968  0.01107318 -0.10881734  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 7.98235123e-01  7.52555262e-04 -5.73105975e-01 -6.33504042e-01]\n",
            "Evaluating reward...\n",
            "In state 18, action Right is worse than Left because it may eventually lead you through state 11 and that is bad.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "0\n",
            "-0.019825146066317127\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.51469351  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Up is -0.02.\n",
            "\n",
            "Sampling demo... \n",
            "12\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.03470968  0.01107318 -0.10796683  0.01830359]\n",
            "Evaluating reward...\n",
            "In state 13 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 8.16597532e-01  7.52555262e-04 -5.73103208e-01 -6.33504042e-01]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is better than Left because it may eventually lead you through state 3 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "17\n",
            "3\n",
            "0.02265640314982122\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 18 when performing action Right is 0.02.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[1.12794103e-05 0.00000000e+00 8.82190528e-04 0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-3.10631801e-03 -4.95469601e-04 -6.36605714e-04  5.12600938e-01]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "17\n",
            "1\n",
            "0.055222508105584614\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 18 when performing action Down is 0.06.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00787505  0.         -0.02621173  0.        ]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-3.10631801e-03 -4.95469601e-04 -6.36605714e-04  5.84334916e-01]\n",
            "Evaluating reward...\n",
            "In state 17, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "4\n",
            "-0.3813395008021906\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action No-Op is -0.38.\n",
            "\n",
            "Sampling demo... \n",
            "5\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 7.88802514e-03 -5.62161958e-05 -2.51965754e-02  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 6 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.75031563e-03 -5.07593831e-04 -6.36605757e-04  6.35444567e-01]\n",
            "Evaluating reward...\n",
            "In state 9, action Up is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "2\n",
            "0.3728178069261384\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Left is 0.37.\n",
            "\n",
            "Sampling demo... \n",
            "1\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 7.89930455e-03 -5.62161958e-05 -2.43143849e-02  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 2 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.75632063e-03 -5.07593831e-04 -6.36605772e-04  6.75948945e-01]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "17\n",
            "3\n",
            "-0.006018062969884453\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 18 when performing action Right is -0.01.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 7.42112038e-03 -5.62161958e-05 -6.17143500e-02  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.75609811e-03 -5.07593831e-04 -6.36605772e-04  7.09257352e-01]\n",
            "Evaluating reward...\n",
            "In state 4, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "1\n",
            "0.040368678964117694\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Down is 0.04.\n",
            "\n",
            "Sampling demo... \n",
            "5\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0074341  -0.00011243 -0.06069919  0.        ]\n",
            "Evaluating reward...\n",
            "In state 6 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.75609811e-03 -5.07593831e-04 -6.36605772e-04  7.37590175e-01]\n",
            "Evaluating reward...\n",
            "In state 14, action Up is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "15\n",
            "4\n",
            "-0.32524727697578304\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 16 when performing action No-Op is -0.33.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0074341  -0.00011243 -0.09809916  0.        ]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.75598713e-03 -5.07593831e-04 -6.36719574e-04  7.62300254e-01]\n",
            "Evaluating reward...\n",
            "In state 13, action Down is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "0\n",
            "0.27849165631963335\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Up is 0.28.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 6.96187807e-03 -1.12432392e-04 -1.35032789e-01  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.74208154e-03 -5.07593831e-04 -6.36719574e-04  7.83959386e-01]\n",
            "Evaluating reward...\n",
            "In state 3, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "1\n",
            "2.330915530333548\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.46618311 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Down is 2.33.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 6.96187807e-03 -1.12432392e-04 -1.72432755e-01  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.13618642e-02 -5.07593831e-04  9.22506376e-03  7.93944411e-01]\n",
            "Evaluating reward...\n",
            "In state 13, action Up is better than Right because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "3\n",
            "0.22731524900034214\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.46618311 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Right is 0.23.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 1.48256443e-02 -1.12432392e-04 -1.99526677e-01  0.00000000e+00]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.13618642e-02 -5.07593831e-04  9.22506376e-03  8.12688608e-01]\n",
            "Evaluating reward...\n",
            "In state 18, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "2\n",
            "0.057116647830065904\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.46618311 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Left is 0.06.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01286268 -0.00217643 -0.19476343  0.        ]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.13563457e-02 -5.07593831e-04  9.21940507e-03  8.29607382e-01]\n",
            "Evaluating reward...\n",
            "In state 13, action Down is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "18\n",
            "4\n",
            "-0.13726695356348922\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.46618311 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 19 when performing action No-Op is -0.14.\n",
            "\n",
            "Sampling demo... \n",
            "14\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01286268 -0.00217643 -0.19384043  0.        ]\n",
            "Evaluating reward...\n",
            "In state 15 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.13526258e-02 -5.07593831e-04  9.21940507e-03  8.45171904e-01]\n",
            "Evaluating reward...\n",
            "In state 4, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "2\n",
            "0.002603060266226526\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.46618311 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Left is 0.00.\n",
            "\n",
            "Sampling demo... \n",
            "4\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01239046 -0.00217643 -0.23077407  0.        ]\n",
            "Evaluating reward...\n",
            "In state 5 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.13527538e-02 -5.07593831e-04  9.21940507e-03  8.59588847e-01]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than Left because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "2\n",
            "0.12148905645818267\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.46618311 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Left is 0.12.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01256086 -0.00217643 -0.24471347  0.01484193]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.13527538e-02 -5.07593831e-04  9.21940507e-03  8.73160642e-01]\n",
            "Evaluating reward...\n",
            "In state 4, action Right is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "7\n",
            "1\n",
            "-0.01427110340106696\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0.         0.         0.46618311 0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 8 when performing action Down is -0.01.\n",
            "\n",
            "Sampling demo... \n",
            "9\n",
            "4\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.01256086 -0.00217643 -0.24451792  0.01484193]\n",
            "Evaluating reward...\n",
            "In state 10 you should perform action No-Op.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 1.13527538e-02 -5.07593831e-04  9.21940507e-03  8.85865602e-01]\n",
            "Evaluating reward...\n",
            "In state 16, action Right is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "5\n",
            "0\n",
            "-0.2774742015598143\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 6 when performing action Up is -0.28.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00196297 -0.002064    0.00476325  0.        ]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00074121 -0.01617479  0.00396157  0.50397972]\n",
            "Evaluating reward...\n",
            "In state 15, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "0\n",
            "2\n",
            "0.013216260435200258\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 1 when performing action Left is 0.01.\n",
            "\n",
            "Sampling demo... \n",
            "3\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00244115 -0.002064   -0.03263672  0.        ]\n",
            "Evaluating reward...\n",
            "In state 4 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00074121 -0.01626761  0.00396157  0.57866155]\n",
            "Evaluating reward...\n",
            "In state 12, action Down is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "1\n",
            "0.3277847979307706\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[0. 0. 0. 0.]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Down is 0.33.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00243001 -0.002064   -0.03176553  0.        ]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00069523 -0.01626761  0.00391565  0.63125873]\n",
            "Evaluating reward...\n",
            "In state 7, action Up is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "9\n",
            "3\n",
            "-1.0292369333071687\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 10 when performing action Right is -1.03.\n",
            "\n",
            "Sampling demo... \n",
            "5\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00241703 -0.00212022 -0.03075037  0.        ]\n",
            "Evaluating reward...\n",
            "In state 6 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-0.00069523 -0.01626761  0.00391565  0.67256907]\n",
            "Evaluating reward...\n",
            "In state 17, action Right is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "3\n",
            "0.28616493665976195\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action Right is 0.29.\n",
            "\n",
            "Sampling demo... \n",
            "5\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00240405 -0.00217643 -0.02973522  0.        ]\n",
            "Evaluating reward...\n",
            "In state 6 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-6.95225231e-04 -1.62676100e-02  3.91565124e-03  7.06416183e-01]\n",
            "Evaluating reward...\n",
            "In state 14, action Up is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "1\n",
            "2\n",
            "0.3440117685264571\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 2 when performing action Left is 0.34.\n",
            "\n",
            "Sampling demo... \n",
            "11\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00436702 -0.00424043 -0.02497197  0.        ]\n",
            "Evaluating reward...\n",
            "In state 12 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-6.90858221e-04 -1.62676100e-02  3.91565124e-03  7.35172318e-01]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "1\n",
            "-0.19163031584891316\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Down is -0.19.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00435588 -0.00424043 -0.02410078  0.        ]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[-6.90858221e-04 -1.62676100e-02  3.91565124e-03  7.59906519e-01]\n",
            "Evaluating reward...\n",
            "In state 8, action Down is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "6\n",
            "2\n",
            "-0.285511515993251\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 7 when performing action Left is -0.29.\n",
            "\n",
            "Sampling demo... \n",
            "15\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[-0.00435588 -0.00424043 -0.02321859  0.        ]\n",
            "Evaluating reward...\n",
            "In state 16 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00425759 -0.01626761  0.00391028  0.78180495]\n",
            "Evaluating reward...\n",
            "In state 10, action Up is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "4\n",
            "4\n",
            "-0.1127097260703764\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 5 when performing action No-Op is -0.11.\n",
            "\n",
            "Sampling demo... \n",
            "2\n",
            "0\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00375415 -0.00424043 -0.03105135  0.        ]\n",
            "Evaluating reward...\n",
            "In state 3 you should perform action Up.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00401239 -0.01626761  0.00391028  0.80170862]\n",
            "Evaluating reward...\n",
            "In state 3, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "11\n",
            "0\n",
            "-0.2752266633173911\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 12 when performing action Up is -0.28.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00392455 -0.00424043 -0.04499076  0.01484193]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00401239 -0.01626761  0.00391028  0.81975185]\n",
            "Evaluating reward...\n",
            "In state 4, action Right is better than Up because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "13\n",
            "1\n",
            "-0.2094623039011431\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 14 when performing action Down is -0.21.\n",
            "\n",
            "Sampling demo... \n",
            "7\n",
            "1\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00345976 -0.00424043 -0.08134353  0.03455504]\n",
            "Evaluating reward...\n",
            "In state 8 you should perform action Down.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.004015   -0.01626761  0.00391028  0.83627844]\n",
            "Evaluating reward...\n",
            "In state 2, action Right is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "17\n",
            "3\n",
            "-0.1766361099181517\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 18 when performing action Right is -0.18.\n",
            "\n",
            "Sampling demo... \n",
            "10\n",
            "2\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.00363016 -0.00424043 -0.09528294  0.04939697]\n",
            "Evaluating reward...\n",
            "In state 11 you should perform action Left.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.004015   -0.01626761  0.00391028  0.85152212]\n",
            "Evaluating reward...\n",
            "In state 15, action Right is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "16\n",
            "2\n",
            "-0.18180811425814775\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.         -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 17 when performing action Left is -0.18.\n",
            "\n",
            "Sampling demo... \n",
            "0\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0036413  -0.00424043 -0.09441175  0.04939697]\n",
            "Evaluating reward...\n",
            "In state 1 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00401513 -0.01627153  0.00391028  0.86549576]\n",
            "Evaluating reward...\n",
            "In state 6, action Up is better than Down because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "8\n",
            "2\n",
            "0.2541327232390824\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.05082654 -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 9 when performing action Left is 0.25.\n",
            "\n",
            "Sampling demo... \n",
            "16\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0036413  -0.00424043 -0.13260918  0.04939697]\n",
            "Evaluating reward...\n",
            "In state 17 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00401513 -0.01627153  0.00391028  0.87869004]\n",
            "Evaluating reward...\n",
            "In state 16, action Right is better than No-Op because it may eventually lead you through state 11 and that is good.\n",
            "Done. Step complete.\n",
            "Sampling reward... \n",
            "12\n",
            "3\n",
            "0.2916960168688812\n",
            "Updating weights for reward... \n",
            "Weights:\n",
            "[ 0.          0.05082654 -0.20584739  0.        ]\n",
            "Evaluating reward...\n",
            "The reward in state 13 when performing action Right is 0.29.\n",
            "\n",
            "Sampling demo... \n",
            "17\n",
            "3\n",
            "Updating weights for demo... \n",
            "Weights:\n",
            "[ 0.0036413  -0.00424043 -0.17000915  0.04939697]\n",
            "Evaluating reward...\n",
            "In state 18 you should perform action Right.\n",
            "\n",
            "Sampling contrastive example... \n",
            "Updating weights for contrastive example... \n",
            "Weights:\n",
            "[ 0.00405971 -0.01627153 -0.50089822  0.87839342]\n",
            "Evaluating reward...\n",
            "In state 7, action Down is worse than Right because it may eventually lead you through state 10 and that is bad.\n",
            "Done. Step complete.\n"
          ]
        }
      ],
      "source": [
        "# Simulation constants\n",
        "NRUNS = 30\n",
        "NSTEPS = 15\n",
        "\n",
        "# For performance statistics\n",
        "perf_rl   = np.zeros((NRUNS, NSTEPS))\n",
        "perf_demo = np.zeros((NRUNS, NSTEPS))\n",
        "perf_expl = np.zeros((NRUNS, NSTEPS))\n",
        "perf_ref  = np.zeros((NRUNS, NSTEPS))\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(40)\n",
        "\n",
        "# Run sims\n",
        "for run in trange(NRUNS):\n",
        "    \n",
        "    # Initialize a random reward\n",
        "    wopt = np.random.randn(len(RFEAT))\n",
        "    ropt = np.sum([wopt[i] * RFEAT[i] for i in range(len(RFEAT))], axis=0)\n",
        "   \n",
        "    #print('Reward:')\n",
        "    #print(np.round(ropt,2))\n",
        "                  \n",
        "    # This is the \"target\" MDP\n",
        "    Mopt = (S, A, P, ropt, gamma)\n",
        "    \n",
        "    # Optimal policy\n",
        "    popt = greedy(vi(Mopt))\n",
        "    \n",
        "    #print('Optimal policy:')\n",
        "    #print(np.round(popt, 2))\n",
        "    \n",
        "    # Weights for 3 approaches\n",
        "    w_rl   = np.zeros(len(RFEAT))\n",
        "    w_demo = np.zeros(len(RFEAT))\n",
        "    w_expl = np.zeros(len(RFEAT))\n",
        "    \n",
        "    samples_expl = []\n",
        "    samples_demos = []\n",
        "    samples_rl = []\n",
        "    # Run test\n",
        "    for t in range(NSTEPS):\n",
        "        #print('\\n[Step %i]' % (t + 1))\n",
        "\n",
        "        # Sample a reward and update\n",
        "        print('Sampling reward... ')\n",
        "        (s, a, r) = sample_reward(Mopt)\n",
        "        print('Updating weights for reward... ')\n",
        "        w_rl = rl_update(w_rl, s, a, r)\n",
        "        print('Weights:')\n",
        "        print(w_rl)\n",
        "\n",
        "        \n",
        "        # Evaluate\n",
        "        print('Evaluating reward...')\n",
        "        perf_rl[run, t] = evaluate_reward(Mopt, w_rl)\n",
        "        samples_rl.append(perf_rl[run, t])\n",
        "        \n",
        "        print(voice_reward(Mopt, s, a, r))\n",
        "        \n",
        "        # Sample a demo and update\n",
        "        print('\\nSampling demo... ')\n",
        "        (s, a) = sample_demo(Mopt)\n",
        "        print('Updating weights for demo... ')\n",
        "        w_demo = demo_update(w_demo, s, a)\n",
        "        print('Weights:')\n",
        "        print(w_demo)\n",
        "\n",
        "        \n",
        "        # Evaluate\n",
        "        print('Evaluating reward...')\n",
        "        perf_demo[run, t] = evaluate_reward(Mopt, w_demo)\n",
        "        samples_demos.append(perf_demo[run, t])\n",
        "        print(voice_demo(Mopt, s, a))\n",
        "        \n",
        "        # Sample an contrastive example and update\n",
        "        print('\\nSampling contrastive example... ')\n",
        "        (s0, s1, a0, a1, k) = sample_explain(Mopt)\n",
        "        \n",
        "        print('Updating weights for contrastive example... ')\n",
        "        w_expl = explain_update(w_expl, s0, s1, a0, a1, k)\n",
        "        print('Weights:')\n",
        "        print(w_expl)\n",
        "        \n",
        "        # Evaluate\n",
        "        #\n",
        "        print('Evaluating reward...')\n",
        "        perf_expl[run, t] = evaluate_reward(Mopt, w_expl)\n",
        "        samples_expl.append(perf_expl[run, t])\n",
        "        print(voice_explain(M, s0, s1, a0, a1, k))\n",
        "        \n",
        "        perf_ref[run, t] = evaluate_reward(Mopt, wopt)\n",
        "        \n",
        "        \n",
        "        print('Done. Step complete.')\n",
        "        \n",
        "# Compute statistics\n",
        "rl_mean = np.mean(perf_rl, axis=0)\n",
        "rl_std = np.std(perf_rl, axis=0) / np.sqrt(NRUNS)\n",
        "\n",
        "demo_mean = np.mean(perf_demo, axis=0)\n",
        "demo_std = np.std(perf_demo, axis=0) / np.sqrt(NRUNS)\n",
        "\n",
        "expl_mean = np.mean(perf_expl, axis=0)\n",
        "expl_std = np.std(perf_expl, axis=0) / np.sqrt(NRUNS)\n",
        "\n",
        "ref_mean = np.mean(perf_ref, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples_expl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s1vF_RpXlY8",
        "outputId": "dd1b29dd-aac4-46f5-a2df-c70d845c2ade"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[82.29851066343026,\n",
              " 82.29851066343026,\n",
              " 82.29851066343026,\n",
              " 82.29851066343026,\n",
              " 82.29851066343026,\n",
              " 82.29851066343026,\n",
              " 82.29851066343026,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples_demos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81AIB4D7Z9Jb",
        "outputId": "5288d3f5-4a37-4eaf-d285-e84c3b788169"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-103.02958273968311,\n",
              " 5.124915531756323,\n",
              " 5.124915531756323,\n",
              " 5.124915531756323,\n",
              " 5.124915531756323,\n",
              " 5.124915531756323,\n",
              " 5.124915531756323,\n",
              " 5.124915531756323,\n",
              " 73.95933762794242,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339,\n",
              " 82.39635481214339]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples_rl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRp21MS2bJuz",
        "outputId": "9806c8ca-5d02-4737-f2b1-1eb97549d149"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.95343359247816,\n",
              " 3.95343359247816,\n",
              " 3.95343359247816,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 10.305091033594506,\n",
              " 18.520448131392154,\n",
              " 18.520448131392154]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rl_min = np.min(rl_mean, axis=0)\n",
        "rl_range = np.max(rl_mean, axis=0) - np.min(rl_mean, axis=0)\n",
        "rl_norm = (rl_mean - rl_min)/rl_range\n",
        "rl_normstd = np.std(rl_norm, axis=0) / np.sqrt(NRUNS)\n",
        "\n",
        "demo_min = np.min(demo_mean, axis=0)\n",
        "demo_range = np.max(demo_mean, axis=0) - np.min(demo_mean, axis=0)\n",
        "demo_norm = (demo_mean - demo_min)/demo_range\n",
        "demo_normstd = np.std(demo_norm, axis=0) / np.sqrt(NRUNS)\n",
        "\n",
        "expl_min = np.min(expl_mean, axis=0)\n",
        "expl_range = np.max(expl_mean, axis=0) - np.min(expl_mean, axis=0)\n",
        "expl_norm = (expl_mean - expl_min)/expl_range\n",
        "expl_normstd = np.std(expl_norm, axis=0) / np.sqrt(NRUNS)\n",
        "\n",
        "\n",
        "ref_min = np.min(ref_mean)\n",
        "ref_range = np.max(ref_mean) - np.min(ref_mean)\n",
        "ref_norm = (ref_mean - ref_min)/ref_range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp7XlfPJSPym",
        "outputId": "a2aed7e0-79d8-43f7-840a-41b878767453"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a6cb8b7f165e>:19: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ref_norm = (ref_mean - ref_min)/ref_range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "d3cHTDXIJ76s",
        "outputId": "6cc77bf6-4cf0-4778-b18e-1d15a2aec076"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(4, 5))\n",
        "plt.fill_between(range(NSTEPS), rl_mean + rl_std, rl_mean - rl_std, color='g', alpha=0.1)\n",
        "plt.fill_between(range(NSTEPS), demo_mean + demo_std, demo_mean - demo_std, color='r', alpha=0.1)\n",
        "plt.fill_between(range(NSTEPS), expl_mean + expl_std, expl_mean - expl_std, color='b', alpha=0.1)\n",
        "plt.plot(range(NSTEPS), rl_mean, 'g:', label='LfR')\n",
        "plt.plot(range(NSTEPS), demo_mean, 'r--', label='LfD')\n",
        "plt.plot(range(NSTEPS), expl_mean, 'b-', label='LfCE')\n",
        "plt.plot(range(NSTEPS), ref_mean, 'k:', label='Optimal')\n",
        "#plt.xlabel('N. samples')\n",
        "#plt.ylabel(r'$||V_\\pi||_2$')\n",
        "plt.xlabel('Teaching signals')\n",
        "plt.ylabel('Average return')\n",
        "plt.ylim(0, 120)\n",
        "plt.xticks(range(0, 15))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure()\n",
        "plt.fill_between(range(NSTEPS), rl_norm*100 + rl_normstd*100, rl_norm*100 - rl_normstd*100, color='g', alpha=0.1)\n",
        "plt.fill_between(range(NSTEPS), demo_norm*100 + demo_normstd*100, demo_norm*100 - demo_normstd*100, color='r', alpha=0.1)\n",
        "plt.fill_between(range(NSTEPS), expl_norm*100 + expl_normstd*100, expl_norm*100 - expl_normstd*100, color='b', alpha=0.1)\n",
        "plt.plot(range(NSTEPS), rl_norm*100, 'g:', label='RL')\n",
        "plt.plot(range(NSTEPS), demo_norm*100, 'r--', label='Demo')\n",
        "plt.plot(range(NSTEPS), expl_norm*100, 'b-', label='Explanation')\n",
        "plt.plot(range(NSTEPS), ref_norm, 'k:', label='Optimal')\n",
        "plt.xlabel('N. samples')\n",
        "plt.ylabel(r'$||V_\\pi||_2$')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "wEPI3fNMSSTN",
        "outputId": "975b6eaa-20dc-401d-e16a-3a18f2499c91"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_VyigwJ76s",
        "outputId": "c2f2d97c-4013-4f32-c1ac-6489574614df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal:\n",
            "[ 0.77400935  0.19217953 -1.08031784  0.86528134]\n",
            "Explanations:\n",
            "[ 0.00405971 -0.01627153 -0.50089822  0.87839342]\n",
            "Demos:\n",
            "[ 0.0036413  -0.00424043 -0.17000915  0.04939697]\n",
            "RL:\n",
            "[ 0.          0.05082654 -0.20584739  0.        ]\n"
          ]
        }
      ],
      "source": [
        "print('Optimal:')\n",
        "print(wopt)\n",
        "print('Explanations:')\n",
        "print(w_expl)\n",
        "print('Demos:')\n",
        "print(w_demo)\n",
        "print('RL:')\n",
        "print(w_rl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wopt = [ 0.77400935, 0.19217953, -1.08031784, 0.86528134]\n",
        "w_expl = [ 0.00405971, -0.01627153, -0.50089822,  0.87839342]\n",
        "w_demos = [-0.20741821, -0.01244667,  0.01091068,  0.05925187]\n",
        "w_rl = [ 0.,0.05082654, -0.20584739, 0.        ]\n"
      ],
      "metadata": {
        "id": "nNU5CvwDF2BN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats\n",
        "from scipy.stats import f_oneway\n",
        "from scipy.stats import normaltest, ttest_rel, shapiro, mannwhitneyu, wilcoxon, ranksums, ttest_ind, kruskal, sem"
      ],
      "metadata": {
        "id": "VQfsUdTyul_5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttest_rel(perf_demo[5], perf_rl[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2xUe-m2e87z",
        "outputId": "694dbf80-2387-4d6a-f282-db8082168732"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TtestResult(statistic=11.152448621050173, pvalue=2.3808155816307443e-08, df=14)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mannwhitneyu(rl_mean,demo_mean)"
      ],
      "metadata": {
        "id": "KOxoPso6utXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84426965-5f2b-4b36-ff9a-6eeb0ed38855"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MannwhitneyuResult(statistic=25.0, pvalue=0.0003078634611799441)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mannwhitneyu(rl_mean,expl_mean)"
      ],
      "metadata": {
        "id": "L2ltypYFu0f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2470b7-3694-43de-8ebc-4f3b26682e01"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MannwhitneyuResult(statistic=0.0, pvalue=3.3833394126542183e-06)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mannwhitneyu(demo_mean,expl_mean)"
      ],
      "metadata": {
        "id": "wXeM5X1gu6o4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbdedbf-d717-4d67-88d2-168efb886494"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MannwhitneyuResult(statistic=4.0, pvalue=7.45976967918418e-06)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ttest_rel(rl_mean,demo_mean)"
      ],
      "metadata": {
        "id": "ueGaHcIQFivE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d588cc6-063f-4793-acee-5435a8538c8a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TtestResult(statistic=-7.164124102861765, pvalue=4.827378202131458e-06, df=14)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ttest_rel(rl_mean,expl_mean)"
      ],
      "metadata": {
        "id": "tYcAfUjWuFHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7056d1-b229-48f1-de67-574bb9722863"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TtestResult(statistic=-11.509656338736663, pvalue=1.5948655358133367e-08, df=14)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ttest_rel(demo_mean,expl_mean)"
      ],
      "metadata": {
        "id": "a5EWFCCrvOhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c1181c-f8a7-4322-dcff-cc602f468536"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TtestResult(statistic=-5.457288234343189, pvalue=8.4449710517586e-05, df=14)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b9c028b12b74788b8502fd92ecfa124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e53844bfc6749d5a922993b572b252f",
              "IPY_MODEL_21ae086e0d964e07ab01977a3a0815c7",
              "IPY_MODEL_3c1fd03ed1014f44bb97971e49053611"
            ],
            "layout": "IPY_MODEL_93c59174246b4ac9ab8a6de1b53eb591"
          }
        },
        "3e53844bfc6749d5a922993b572b252f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac2cbfefa53c4b3f93d79f97cb69c5ff",
            "placeholder": "",
            "style": "IPY_MODEL_f1e050c638dc4a5f8f2abd6786798fce",
            "value": "100%"
          }
        },
        "21ae086e0d964e07ab01977a3a0815c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_730fd69f7f644a9fbf90f8f30a1fface",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcfc92b369234d22bf5db17165ff6774",
            "value": 30
          }
        },
        "3c1fd03ed1014f44bb97971e49053611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_572d23dc8b98451098ddfa7dd13454e5",
            "placeholder": "",
            "style": "IPY_MODEL_990567b6d90d401c896ef2af9cc26e8d",
            "value": " 30/30 [08:00&lt;00:00, 15.14s/it]"
          }
        },
        "93c59174246b4ac9ab8a6de1b53eb591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac2cbfefa53c4b3f93d79f97cb69c5ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e050c638dc4a5f8f2abd6786798fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "730fd69f7f644a9fbf90f8f30a1fface": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfc92b369234d22bf5db17165ff6774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "572d23dc8b98451098ddfa7dd13454e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "990567b6d90d401c896ef2af9cc26e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}